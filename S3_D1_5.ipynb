{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1",
      "mount_file_id": "1yji5sYrzseEeECTzQGB9eMpYflMrNei_",
      "authorship_tag": "ABX9TyN42awGkIwd5yPx12raJn3z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xtian76/-MS-Progression-Prediction/blob/main/S3_D1_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_hjLa8TQqSm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d69a26a",
        "outputId": "b26f2237-35c3-42c1-977e-709a04db6e55"
      },
      "source": [
        "import os\n",
        "\n",
        "extract_path = '/content/ISBI_dataset'\n",
        "\n",
        "# List files and directories in the extracted path\n",
        "for root, dirs, files in os.walk(extract_path):\n",
        "    level = root.replace(extract_path, '').count(os.sep)\n",
        "    indent = ' ' * 4 * (level)\n",
        "    print(f'{indent}{os.path.basename(root)}/')\n",
        "    subindent = ' ' * 4 * (level + 1)\n",
        "    for f in files:\n",
        "        print(f'{subindent}{f}')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ISBI_dataset/\n",
            "    training/\n",
            "        license.txt\n",
            "        license.txt.bak\n",
            "        2017-NI-Carass-Longitudinal-multiple-sclerosis-lesion-segmentation-Resource-and-challenge.pdf\n",
            "        training03/\n",
            "            orig/\n",
            "                training03_02_mprage.nii\n",
            "                training03_03_mprage.nii\n",
            "                training03_03_t2.nii\n",
            "                training03_03_pd.nii\n",
            "                training03_05_flair.nii\n",
            "                training03_04_mprage.nii\n",
            "                training03_04_pd.nii\n",
            "                training03_01_mprage.nii\n",
            "                training03_04_flair.nii\n",
            "                training03_05_t2.nii\n",
            "                training03_02_flair.nii\n",
            "                training03_01_pd.nii\n",
            "                training03_03_flair.nii\n",
            "                training03_05_pd.nii\n",
            "                training03_02_t2.nii\n",
            "                training03_01_flair.nii\n",
            "                training03_05_mprage.nii\n",
            "                training03_04_t2.nii\n",
            "                training03_01_t2.nii\n",
            "                training03_02_pd.nii\n",
            "            preprocessed/\n",
            "                training03_01_t2_pp.nii\n",
            "                training03_01_pd_pp.nii\n",
            "                training03_05_pd_pp.nii\n",
            "                training03_05_t2_pp.nii\n",
            "                training03_02_pd_pp.nii\n",
            "                training03_01_flair_pp.nii\n",
            "                training03_04_mprage_pp.nii\n",
            "                training03_02_t2_pp.nii\n",
            "                training03_04_pd_pp.nii\n",
            "                training03_05_flair_pp.nii\n",
            "                training03_01_mprage_pp.nii\n",
            "                training03_03_flair_pp.nii\n",
            "                training03_03_mprage_pp.nii\n",
            "                training03_04_t2_pp.nii\n",
            "                training03_03_t2_pp.nii\n",
            "                training03_03_pd_pp.nii\n",
            "                training03_05_mprage_pp.nii\n",
            "                training03_04_flair_pp.nii\n",
            "                training03_02_flair_pp.nii\n",
            "                training03_02_mprage_pp.nii\n",
            "            masks/\n",
            "                training03_02_mask1.nii\n",
            "                training03_04_mask2.nii\n",
            "                training03_05_mask1.nii\n",
            "                training03_01_mask1.nii\n",
            "                training03_05_mask2.nii\n",
            "                training03_04_mask1.nii\n",
            "                training03_03_mask2.nii\n",
            "                training03_01_mask2.nii\n",
            "                training03_03_mask1.nii\n",
            "                training03_02_mask2.nii\n",
            "        training05/\n",
            "            orig/\n",
            "                training05_02_flair.nii\n",
            "                training05_02_mprage.nii\n",
            "                training05_02_t2.nii\n",
            "                training05_03_pd.nii\n",
            "                training05_01_mprage.nii\n",
            "                training05_04_mprage.nii\n",
            "                training05_01_pd.nii\n",
            "                training05_01_t2.nii\n",
            "                training05_04_flair.nii\n",
            "                training05_04_t2.nii\n",
            "                training05_04_pd.nii\n",
            "                training05_02_pd.nii\n",
            "                training05_03_mprage.nii\n",
            "                training05_03_t2.nii\n",
            "                training05_03_flair.nii\n",
            "                training05_01_flair.nii\n",
            "            preprocessed/\n",
            "                training05_04_t2_pp.nii\n",
            "                training05_03_t2_pp.nii\n",
            "                training05_01_pd_pp.nii\n",
            "                training05_03_pd_pp.nii\n",
            "                training05_04_pd_pp.nii\n",
            "                training05_02_pd_pp.nii\n",
            "                training05_03_flair_pp.nii\n",
            "                training05_02_flair_pp.nii\n",
            "                training05_01_mprage_pp.nii\n",
            "                training05_04_mprage_pp.nii\n",
            "                training05_01_flair_pp.nii\n",
            "                training05_04_flair_pp.nii\n",
            "                training05_03_mprage_pp.nii\n",
            "                training05_02_mprage_pp.nii\n",
            "                training05_02_t2_pp.nii\n",
            "                training05_01_t2_pp.nii\n",
            "            masks/\n",
            "                training05_04_mask2.nii\n",
            "                training05_03_mask2.nii\n",
            "                training05_01_mask1.nii\n",
            "                training05_04_mask1.nii\n",
            "                training05_03_mask1.nii\n",
            "                training05_01_mask2.nii\n",
            "                training05_02_mask1.nii\n",
            "                training05_02_mask2.nii\n",
            "        training01/\n",
            "            orig/\n",
            "                training01_01_flair.nii\n",
            "                training01_01_mprage.nii\n",
            "                training01_04_t2.nii\n",
            "                training01_04_pd.nii\n",
            "                training01_03_t2.nii\n",
            "                training01_01_t2.nii\n",
            "                training01_03_flair.nii\n",
            "                training01_04_mprage.nii\n",
            "                training01_03_mprage.nii\n",
            "                training01_02_t2.nii\n",
            "                training01_04_flair.nii\n",
            "                training01_02_flair.nii\n",
            "                training01_02_pd.nii\n",
            "                training01_02_mprage.nii\n",
            "                training01_03_pd.nii\n",
            "                training01_01_pd.nii\n",
            "            preprocessed/\n",
            "                training01_04_pd_pp.nii\n",
            "                training01_04_mprage_pp.nii\n",
            "                training01_03_pd_pp.nii\n",
            "                training01_03_mprage_pp.nii\n",
            "                training01_02_flair_pp.nii\n",
            "                training01_04_flair_pp.nii\n",
            "                training01_02_t2_pp.nii\n",
            "                training01_01_pd_pp.nii\n",
            "                training01_03_flair_pp.nii\n",
            "                training01_01_mprage_pp.nii\n",
            "                training01_02_mprage_pp.nii\n",
            "                training01_01_flair_pp.nii\n",
            "                training01_02_pd_pp.nii\n",
            "                training01_01_t2_pp.nii\n",
            "                training01_04_t2_pp.nii\n",
            "                training01_03_t2_pp.nii\n",
            "            masks/\n",
            "                training01_02_mask2.nii\n",
            "                training01_03_mask1.nii\n",
            "                training01_02_mask1.nii\n",
            "                training01_03_mask2.nii\n",
            "                training01_01_mask1.nii\n",
            "                training01_04_mask2.nii\n",
            "                training01_04_mask1.nii\n",
            "                training01_01_mask2.nii\n",
            "        training04/\n",
            "            orig/\n",
            "                training04_02_flair.nii\n",
            "                training04_04_pd.nii\n",
            "                training04_02_mprage.nii\n",
            "                training04_01_mprage.nii\n",
            "                training04_02_pd.nii\n",
            "                training04_01_t2.nii\n",
            "                training04_03_t2.nii\n",
            "                training04_02_t2.nii\n",
            "                training04_03_mprage.nii\n",
            "                training04_04_mprage.nii\n",
            "                training04_01_flair.nii\n",
            "                training04_04_t2.nii\n",
            "                training04_01_pd.nii\n",
            "                training04_04_flair.nii\n",
            "                training04_03_pd.nii\n",
            "                training04_03_flair.nii\n",
            "            preprocessed/\n",
            "                training04_03_t2_pp.nii\n",
            "                training04_02_t2_pp.nii\n",
            "                training04_01_flair_pp.nii\n",
            "                training04_04_pd_pp.nii\n",
            "                training04_02_pd_pp.nii\n",
            "                training04_03_pd_pp.nii\n",
            "                training04_02_mprage_pp.nii\n",
            "                training04_01_t2_pp.nii\n",
            "                training04_01_mprage_pp.nii\n",
            "                training04_01_pd_pp.nii\n",
            "                training04_02_flair_pp.nii\n",
            "                training04_03_flair_pp.nii\n",
            "                training04_04_t2_pp.nii\n",
            "                training04_04_flair_pp.nii\n",
            "                training04_04_mprage_pp.nii\n",
            "                training04_03_mprage_pp.nii\n",
            "            masks/\n",
            "                training04_02_mask1.nii\n",
            "                training04_01_mask2.nii\n",
            "                training04_04_mask2.nii\n",
            "                training04_01_mask1.nii\n",
            "                training04_03_mask1.nii\n",
            "                training04_04_mask1.nii\n",
            "                training04_03_mask2.nii\n",
            "                training04_02_mask2.nii\n",
            "        training02/\n",
            "            orig/\n",
            "                training02_03_flair.nii\n",
            "                training02_04_pd.nii\n",
            "                training02_01_t2.nii\n",
            "                training02_04_mprage.nii\n",
            "                training02_02_mprage.nii\n",
            "                training02_04_flair.nii\n",
            "                training02_02_pd.nii\n",
            "                training02_03_t2.nii\n",
            "                training02_01_pd.nii\n",
            "                training02_03_pd.nii\n",
            "                training02_01_mprage.nii\n",
            "                training02_02_flair.nii\n",
            "                training02_03_mprage.nii\n",
            "                training02_02_t2.nii\n",
            "                training02_04_t2.nii\n",
            "                training02_01_flair.nii\n",
            "            preprocessed/\n",
            "                training02_01_pd_pp.nii\n",
            "                training02_02_mprage_pp.nii\n",
            "                training02_01_mprage_pp.nii\n",
            "                training02_03_flair_pp.nii\n",
            "                training02_04_pd_pp.nii\n",
            "                training02_01_t2_pp.nii\n",
            "                training02_03_t2_pp.nii\n",
            "                training02_04_t2_pp.nii\n",
            "                training02_02_flair_pp.nii\n",
            "                training02_04_flair_pp.nii\n",
            "                training02_02_pd_pp.nii\n",
            "                training02_03_pd_pp.nii\n",
            "                training02_04_mprage_pp.nii\n",
            "                training02_03_mprage_pp.nii\n",
            "                training02_01_flair_pp.nii\n",
            "                training02_02_t2_pp.nii\n",
            "            masks/\n",
            "                training02_03_mask1.nii\n",
            "                training02_04_mask1.nii\n",
            "                training02_01_mask1.nii\n",
            "                training02_02_mask2.nii\n",
            "                training02_03_mask2.nii\n",
            "                training02_02_mask1.nii\n",
            "                training02_01_mask2.nii\n",
            "                training02_04_mask2.nii\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fd78278",
        "outputId": "442cea40-1fb1-4002-a3e1-03bf9651b5b8"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/TFM ISBI/ISBI 2015 dataset.zip'\n",
        "extract_path = '/content/ISBI_dataset'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "if not os.path.exists(extract_path):\n",
        "    os.makedirs(extract_path)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Archivo zip extraído a: {extract_path}\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo zip extraído a: /content/ISBI_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d71809a9"
      },
      "source": [
        "# Task\n",
        "Perform image data preparation on the dataset located at \"/content/drive/MyDrive/TFM ISBI/ISBI 2015 dataset.zip\" following these steps: Create a dataset for Deep Learning with a defined input size (e.g., 128x128x128), uniform resizing/cropping, per-patient normalization, and 3D data augmentation. Implement a custom DataLoader for efficient memory loading, batch generation, and on-the-fly augmentations. Perform a stratified split (Train/Val/Test) by phenotype, ensuring balanced representation and considering the longitudinal aspect. Generate the code to load, resize, and normalize the images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ae3098a"
      },
      "source": [
        "## Importar bibliotecas necesarias\n",
        "\n",
        "### Subtask:\n",
        "Import necessary libraries for loading and manipulating NIfTI images and numerical arrays.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c05db3c"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary libraries for handling NIfTI files and numerical operations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85703ff9"
      },
      "source": [
        "import nibabel as nb\n",
        "import numpy as np"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c2917e5"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the missing library `nibabel` using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UefA2bWSYjF",
        "outputId": "ca3d8846-ba11-46db-ee45-1ede62f7814c"
      },
      "source": [
        "%pip install nibabel"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.12/dist-packages (5.3.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from nibabel) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from nibabel) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.12/dist-packages (from nibabel) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e5135a1"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `nibabel` is installed, import the necessary libraries for handling NIfTI files and numerical operations again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63b2b9a5"
      },
      "source": [
        "import nibabel as nb\n",
        "import numpy as np"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "532d3107"
      },
      "source": [
        "## Iterar sobre los directorios de pacientes\n",
        "\n",
        "### Subtask:\n",
        "Iterate over the directories of each patient to access their medical images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f9566ed"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate over the directories of each patient to access their medical images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47c6a922",
        "outputId": "e9391b94-6613-45f9-e54b-6782f715b7c4"
      },
      "source": [
        "base_path = '/content/ISBI_dataset/training'\n",
        "patient_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
        "\n",
        "for patient_dir in patient_dirs:\n",
        "    print(f\"Processing patient: {patient_dir}\")\n",
        "    # Further processing for each patient will be added here"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing patient: training03\n",
            "Processing patient: training05\n",
            "Processing patient: training01\n",
            "Processing patient: training04\n",
            "Processing patient: training02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f825762"
      },
      "source": [
        "## Cargar imágenes\n",
        "\n",
        "### Subtask:\n",
        "Load the relevant NIfTI image files for each patient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff96612b"
      },
      "source": [
        "**Reasoning**:\n",
        "Construct the path to the preprocessed directory, list and filter the NIfTI files, and load them for each patient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9431c78e",
        "outputId": "2fade84e-b071-4a66-c428-02c0726b190b"
      },
      "source": [
        "loaded_images = {}\n",
        "\n",
        "for patient_dir in patient_dirs:\n",
        "    print(f\"Processing patient: {patient_dir}\")\n",
        "    preprocessed_path = os.path.join(base_path, patient_dir, 'preprocessed')\n",
        "\n",
        "    if os.path.exists(preprocessed_path):\n",
        "        nii_files = [f for f in os.listdir(preprocessed_path) if f.endswith('.nii')]\n",
        "        patient_images = []\n",
        "        for nii_file in nii_files:\n",
        "            file_path = os.path.join(preprocessed_path, nii_file)\n",
        "            try:\n",
        "                img = nb.load(file_path)\n",
        "                patient_images.append(img)\n",
        "                print(f\"  Loaded: {nii_file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error loading {nii_file}: {e}\")\n",
        "        loaded_images[patient_dir] = patient_images\n",
        "    else:\n",
        "        print(f\"  Preprocessed directory not found for patient: {patient_dir}\")\n",
        "\n",
        "print(\"\\nFinished loading images for all patients.\")\n",
        "# Optional: Display the number of images loaded per patient\n",
        "for patient, images in loaded_images.items():\n",
        "    print(f\"Patient {patient}: {len(images)} images loaded\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing patient: training03\n",
            "  Loaded: training03_01_t2_pp.nii\n",
            "  Loaded: training03_01_pd_pp.nii\n",
            "  Loaded: training03_05_pd_pp.nii\n",
            "  Loaded: training03_05_t2_pp.nii\n",
            "  Loaded: training03_02_pd_pp.nii\n",
            "  Loaded: training03_01_flair_pp.nii\n",
            "  Loaded: training03_04_mprage_pp.nii\n",
            "  Loaded: training03_02_t2_pp.nii\n",
            "  Loaded: training03_04_pd_pp.nii\n",
            "  Loaded: training03_05_flair_pp.nii\n",
            "  Loaded: training03_01_mprage_pp.nii\n",
            "  Loaded: training03_03_flair_pp.nii\n",
            "  Loaded: training03_03_mprage_pp.nii\n",
            "  Loaded: training03_04_t2_pp.nii\n",
            "  Loaded: training03_03_t2_pp.nii\n",
            "  Loaded: training03_03_pd_pp.nii\n",
            "  Loaded: training03_05_mprage_pp.nii\n",
            "  Loaded: training03_04_flair_pp.nii\n",
            "  Loaded: training03_02_flair_pp.nii\n",
            "  Loaded: training03_02_mprage_pp.nii\n",
            "Processing patient: training05\n",
            "  Loaded: training05_04_t2_pp.nii\n",
            "  Loaded: training05_03_t2_pp.nii\n",
            "  Loaded: training05_01_pd_pp.nii\n",
            "  Loaded: training05_03_pd_pp.nii\n",
            "  Loaded: training05_04_pd_pp.nii\n",
            "  Loaded: training05_02_pd_pp.nii\n",
            "  Loaded: training05_03_flair_pp.nii\n",
            "  Loaded: training05_02_flair_pp.nii\n",
            "  Loaded: training05_01_mprage_pp.nii\n",
            "  Loaded: training05_04_mprage_pp.nii\n",
            "  Loaded: training05_01_flair_pp.nii\n",
            "  Loaded: training05_04_flair_pp.nii\n",
            "  Loaded: training05_03_mprage_pp.nii\n",
            "  Loaded: training05_02_mprage_pp.nii\n",
            "  Loaded: training05_02_t2_pp.nii\n",
            "  Loaded: training05_01_t2_pp.nii\n",
            "Processing patient: training01\n",
            "  Loaded: training01_04_pd_pp.nii\n",
            "  Loaded: training01_04_mprage_pp.nii\n",
            "  Loaded: training01_03_pd_pp.nii\n",
            "  Loaded: training01_03_mprage_pp.nii\n",
            "  Loaded: training01_02_flair_pp.nii\n",
            "  Loaded: training01_04_flair_pp.nii\n",
            "  Loaded: training01_02_t2_pp.nii\n",
            "  Loaded: training01_01_pd_pp.nii\n",
            "  Loaded: training01_03_flair_pp.nii\n",
            "  Loaded: training01_01_mprage_pp.nii\n",
            "  Loaded: training01_02_mprage_pp.nii\n",
            "  Loaded: training01_01_flair_pp.nii\n",
            "  Loaded: training01_02_pd_pp.nii\n",
            "  Loaded: training01_01_t2_pp.nii\n",
            "  Loaded: training01_04_t2_pp.nii\n",
            "  Loaded: training01_03_t2_pp.nii\n",
            "Processing patient: training04\n",
            "  Loaded: training04_03_t2_pp.nii\n",
            "  Loaded: training04_02_t2_pp.nii\n",
            "  Loaded: training04_01_flair_pp.nii\n",
            "  Loaded: training04_04_pd_pp.nii\n",
            "  Loaded: training04_02_pd_pp.nii\n",
            "  Loaded: training04_03_pd_pp.nii\n",
            "  Loaded: training04_02_mprage_pp.nii\n",
            "  Loaded: training04_01_t2_pp.nii\n",
            "  Loaded: training04_01_mprage_pp.nii\n",
            "  Loaded: training04_01_pd_pp.nii\n",
            "  Loaded: training04_02_flair_pp.nii\n",
            "  Loaded: training04_03_flair_pp.nii\n",
            "  Loaded: training04_04_t2_pp.nii\n",
            "  Loaded: training04_04_flair_pp.nii\n",
            "  Loaded: training04_04_mprage_pp.nii\n",
            "  Loaded: training04_03_mprage_pp.nii\n",
            "Processing patient: training02\n",
            "  Loaded: training02_01_pd_pp.nii\n",
            "  Loaded: training02_02_mprage_pp.nii\n",
            "  Loaded: training02_01_mprage_pp.nii\n",
            "  Loaded: training02_03_flair_pp.nii\n",
            "  Loaded: training02_04_pd_pp.nii\n",
            "  Loaded: training02_01_t2_pp.nii\n",
            "  Loaded: training02_03_t2_pp.nii\n",
            "  Loaded: training02_04_t2_pp.nii\n",
            "  Loaded: training02_02_flair_pp.nii\n",
            "  Loaded: training02_04_flair_pp.nii\n",
            "  Loaded: training02_02_pd_pp.nii\n",
            "  Loaded: training02_03_pd_pp.nii\n",
            "  Loaded: training02_04_mprage_pp.nii\n",
            "  Loaded: training02_03_mprage_pp.nii\n",
            "  Loaded: training02_01_flair_pp.nii\n",
            "  Loaded: training02_02_t2_pp.nii\n",
            "\n",
            "Finished loading images for all patients.\n",
            "Patient training03: 20 images loaded\n",
            "Patient training05: 16 images loaded\n",
            "Patient training01: 16 images loaded\n",
            "Patient training04: 16 images loaded\n",
            "Patient training02: 16 images loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "176193d5"
      },
      "source": [
        "## Redimensionar/recortar (resizing/cropping)\n",
        "\n",
        "### Subtask:\n",
        "Implement a function to resize or crop the loaded images to a uniform size (e.g., 128x128x128).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83687f31"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the target size and a function to resize/crop the image data, then iterate through the loaded images and apply the function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bd1e580",
        "outputId": "4213ad78-2fdc-40e5-9090-976002d52093"
      },
      "source": [
        "target_size = (128, 128, 128)\n",
        "\n",
        "def resize_or_crop_image(image_obj, target_size):\n",
        "    \"\"\"\n",
        "    Resizes or crops a Nibabel image object to a target size.\n",
        "\n",
        "    Args:\n",
        "        image_obj: A Nibabel image object.\n",
        "        target_size: A tuple representing the target size (depth, height, width).\n",
        "\n",
        "    Returns:\n",
        "        A NumPy array of the resized/cropped image data.\n",
        "    \"\"\"\n",
        "    image_data = image_obj.get_fdata()\n",
        "    current_size = image_data.shape\n",
        "\n",
        "    # Calculate padding or cropping\n",
        "    padding = [(max(0, (target_size[i] - current_size[i]) // 2), max(0, target_size[i] - current_size[i] - (target_size[i] - current_size[i]) // 2)) for i in range(3)]\n",
        "    cropping = [(max(0, (current_size[i] - target_size[i]) // 2), max(0, current_size[i] - target_size[i] - (current_size[i] - target_size[i]) // 2)) for i in range(3)]\n",
        "\n",
        "    # Apply padding\n",
        "    if any(p[0] > 0 or p[1] > 0 for p in padding):\n",
        "        image_data = np.pad(image_data, padding, mode='constant')\n",
        "\n",
        "    # Apply cropping\n",
        "    if any(c[0] > 0 or c[1] > 0 for c in cropping):\n",
        "        image_data = image_data[cropping[0][0]:current_size[0]-cropping[0][1],\n",
        "                                 cropping[1][0]:current_size[1]-cropping[1][1],\n",
        "                                 cropping[2][0]:current_size[2]-cropping[2][1]]\n",
        "\n",
        "    # If the image is still larger than the target after centering the crop,\n",
        "    # take a centered slice of the target size. This can happen if the\n",
        "    # dimension is an odd number and the target is even, for example.\n",
        "    if image_data.shape != target_size:\n",
        "         start_indices = [(image_data.shape[i] - target_size[i]) // 2 for i in range(3)]\n",
        "         end_indices = [start_indices[i] + target_size[i] for i in range(3)]\n",
        "         image_data = image_data[start_indices[0]:end_indices[0],\n",
        "                                  start_indices[1]:end_indices[1],\n",
        "                                  start_indices[2]:end_indices[2]]\n",
        "\n",
        "    return image_data\n",
        "\n",
        "resized_images = {}\n",
        "\n",
        "for patient_dir, images in loaded_images.items():\n",
        "    resized_images[patient_dir] = []\n",
        "    for img in images:\n",
        "        resized_img_data = resize_or_crop_image(img, target_size)\n",
        "        resized_images[patient_dir].append(resized_img_data)\n",
        "        print(f\"Resized/Cropped image for {patient_dir} with shape: {resized_img_data.shape}\")\n",
        "\n",
        "print(\"\\nFinished resizing/cropping images for all patients.\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training03 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training05 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training05 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training05 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training05 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training05 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training05 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training05 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training05 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training05 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training05 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training05 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training05 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training05 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training05 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training05 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training05 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training01 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training01 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training01 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training01 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training01 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training01 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training01 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training01 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training01 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training01 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training01 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training01 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training01 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training01 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training01 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training01 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training04 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training04 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training04 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training04 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training04 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training04 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training04 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training04 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training04 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training04 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training04 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training04 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training04 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training04 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training04 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training04 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training02 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training02 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training02 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training02 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training02 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training02 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training02 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training02 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training02 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training02 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training02 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training02 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training02 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training02 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training02 with shape: (128, 128, 128)\n",
            "Resized/Cropped image for training02 with shape: (128, 128, 128)\n",
            "\n",
            "Finished resizing/cropping images for all patients.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1a03a7e"
      },
      "source": [
        "## Normalización por paciente\n",
        "\n",
        "### Subtask:\n",
        "Implement a function to normalize the image data for each patient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0efcbf4"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the normalization function and apply it to the resized images for each patient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfa60546",
        "outputId": "a17204e7-268e-4014-87c3-08343ab8d5a9"
      },
      "source": [
        "def normalize_patient_images(image_list):\n",
        "    \"\"\"\n",
        "    Applies Z-score normalization to a list of image data arrays for a single patient.\n",
        "\n",
        "    Args:\n",
        "        image_list: A list of NumPy arrays representing image data for a patient.\n",
        "\n",
        "    Returns:\n",
        "        A list of NumPy arrays with normalized image data.\n",
        "    \"\"\"\n",
        "    if not image_list:\n",
        "        return []\n",
        "\n",
        "    # Calculate mean and standard deviation across all images for the patient\n",
        "    all_patient_data = np.concatenate([img.flatten() for img in image_list])\n",
        "    mean = np.mean(all_patient_data)\n",
        "    std = np.std(all_patient_data)\n",
        "\n",
        "    normalized_image_list = []\n",
        "    for img_data in image_list:\n",
        "        # Apply Z-score normalization\n",
        "        normalized_img_data = (img_data - mean) / (std + 1e-8)  # Add small epsilon to avoid division by zero\n",
        "        normalized_image_list.append(normalized_img_data)\n",
        "\n",
        "    return normalized_image_list\n",
        "\n",
        "normalized_images = {}\n",
        "\n",
        "for patient_dir, images in resized_images.items():\n",
        "    print(f\"Normalizing images for patient: {patient_dir}\")\n",
        "    normalized_patient_images = normalize_patient_images(images)\n",
        "    normalized_images[patient_dir] = normalized_patient_images\n",
        "    print(f\"Finished normalizing images for patient: {patient_dir}\")\n",
        "\n",
        "print(\"\\nFinished normalizing images for all patients.\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalizing images for patient: training03\n",
            "Finished normalizing images for patient: training03\n",
            "Normalizing images for patient: training05\n",
            "Finished normalizing images for patient: training05\n",
            "Normalizing images for patient: training01\n",
            "Finished normalizing images for patient: training01\n",
            "Normalizing images for patient: training04\n",
            "Finished normalizing images for patient: training04\n",
            "Normalizing images for patient: training02\n",
            "Finished normalizing images for patient: training02\n",
            "\n",
            "Finished normalizing images for all patients.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52256bc0"
      },
      "source": [
        "## Almacenar imágenes preprocesadas\n",
        "\n",
        "### Subtask:\n",
        "Store the preprocessed (resized and normalized) images in a suitable format for later use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe70dd30"
      },
      "source": [
        "## Confirmar preprocesamiento\n",
        "\n",
        "### Subtask:\n",
        "Display an example of a preprocessed image to visually confirm that the resizing, cropping, and normalization steps were performed correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46113f9a"
      },
      "source": [
        "**Reasoning**:\n",
        "Display a slice of a preprocessed image to visually verify the resizing, cropping, and normalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "6005c17b",
        "outputId": "56c11ac8-1c72-48db-8fe2-33fce0855a96"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select the first patient and the first image\n",
        "patient_key = list(normalized_images.keys())[0]\n",
        "image_array = normalized_images[patient_key][0]\n",
        "\n",
        "# Choose a slice to display (e.g., the middle slice along the first dimension)\n",
        "slice_index = image_array.shape[0] // 2\n",
        "image_slice = image_array[slice_index, :, :]\n",
        "\n",
        "# Display the slice\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(image_slice, cmap='gray')\n",
        "plt.colorbar(label='Normalized Intensity')\n",
        "plt.title(f'Preprocessed Image Slice for Patient: {patient_key}, Slice: {slice_index}')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1c58526"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The necessary libraries, `nibabel` and `numpy`, were successfully imported after installing `nibabel`.\n",
        "*   The code successfully iterated through patient directories within the training dataset.\n",
        "*   All relevant NIfTI image files were loaded for each patient from the 'preprocessed' subdirectories.\n",
        "*   A function was implemented to resize or crop images to a uniform size of 128x128x128, successfully handling both padding and cropping.\n",
        "*   Per-patient Z-score normalization was implemented and applied to the resized image data, successfully normalizing the intensity values for each patient.\n",
        "*   The preprocessed images (resized and normalized) were stored in a Python dictionary.\n",
        "*   A visual confirmation step successfully displayed a slice of a preprocessed image, showing normalized intensity values.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The preprocessed image data is now ready for the next steps, including implementing a custom DataLoader, performing stratified data splitting, and applying 3D data augmentation.\n",
        "*   The current storage in a dictionary is suitable for immediate use, but for larger datasets, consider more memory-efficient storage methods like HDF5 files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "158acbbe"
      },
      "source": [
        "# Task\n",
        "Realiza una preparación de datos de imagen del dataset que se encuentra en la ruta \"/content/drive/MyDrive/TFM ISBI/ISBI 2015 dataset.zip\", siguiendo las siguientes tareas: Crear dataset de imágenes para Deep Learning: Definir tamaño de entrada (ej: 128×128×128), Resizing/cropping uniforme, Normalización por paciente, Data augmentation 3D. Implementar DataLoader personalizado: Carga eficiente en memoria, Batch generation, Augmentations on-the-fly. Split estratificado: Train/Val/Test por fenotipo, Asegurar representación balanceada, Considerar aspecto longitudinal. Genera el código para cargar, redimensionar y normalizar las imágenes. Implementar DataLoader personalizado: Carga eficiente en memoria, Batch generation, Augmentations on-the-fly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dab0124"
      },
      "source": [
        "## Definir la clase dataset\n",
        "\n",
        "### Subtask:\n",
        "Crear una clase personalizada que herede de `torch.utils.data.Dataset`. Esta clase se encargará de cargar una sola muestra de datos (imagen y su máscara si está disponible) y aplicar las transformaciones necesarias.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4888b20d"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary PyTorch library and define the `ISBIDataset` class with its `__init__` method to store image paths, mask paths, and transformations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e5b41f6"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ISBIDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for ISBI 2015 challenge data.\"\"\"\n",
        "\n",
        "    def __init__(self, image_paths, mask_paths=None, transforms=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_paths (list): List of paths to the image files.\n",
        "            mask_paths (list, optional): List of paths to the mask files. Defaults to None.\n",
        "            transforms (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        # This method will be implemented in the next subtask\n",
        "        pass\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # This method will be implemented in the next subtask\n",
        "        pass\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55c62127"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the `__len__` method to return the total number of samples in the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59b1c459"
      },
      "source": [
        "class ISBIDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for ISBI 2015 challenge data.\"\"\"\n",
        "\n",
        "    def __init__(self, image_paths, mask_paths=None, transforms=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_paths (list): List of paths to the image files.\n",
        "            mask_paths (list, optional): List of paths to the mask files. Defaults to None.\n",
        "            transforms (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples.\"\"\"\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # This method will be implemented in the next subtask\n",
        "        pass"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4bf4d04"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the `__getitem__` method to load a single sample (image and mask if available) from the dataset and apply transformations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "073cc5de"
      },
      "source": [
        "import nibabel as nib # Using 'nib' as a shorthand\n",
        "import numpy as np\n",
        "\n",
        "class ISBIDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for ISBI 2015 challenge data.\"\"\"\n",
        "\n",
        "    def __init__(self, image_paths, mask_paths=None, transforms=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_paths (list): List of paths to the image files.\n",
        "            mask_paths (list, optional): List of paths to the mask files. Defaults to None.\n",
        "            transforms (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples.\"\"\"\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Loads and returns a sample from the dataset at the given index.\"\"\"\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = nib.load(image_path).get_fdata()\n",
        "        image = np.expand_dims(image, axis=0) # Add channel dimension\n",
        "\n",
        "        sample = {'image': image}\n",
        "\n",
        "        if self.mask_paths:\n",
        "            mask_path = self.mask_paths[idx]\n",
        "            mask = nib.load(mask_path).get_fdata()\n",
        "            mask = np.expand_dims(mask, axis=0) # Add channel dimension\n",
        "            sample['mask'] = mask\n",
        "\n",
        "        if self.transforms:\n",
        "            sample = self.transforms(sample)\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19129b9c"
      },
      "source": [
        "## Definir la clase dataloader\n",
        "\n",
        "### Subtask:\n",
        "Utilizar `torch.utils.data.DataLoader` with the custom Dataset class to handle batch loading, shuffling, and parallel loading.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6a5501f"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the DataLoader class from torch.utils.data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b39ef78"
      },
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec67721a"
      },
      "source": [
        "**Reasoning**:\n",
        "Create an instance of the custom dataset and the DataLoader.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a60c5684",
        "outputId": "bc9e37c2-b816-444b-fd2a-e064fbf450e5"
      },
      "source": [
        "# Assuming 'normalized_images' contains the preprocessed image data\n",
        "# We need to create a list of image paths to pass to the Dataset.\n",
        "# Since the images are already loaded and preprocessed in memory,\n",
        "# we will create a dummy list of paths or adapt the Dataset to take the data directly.\n",
        "# For the purpose of demonstrating the DataLoader, let's assume we have a list of paths.\n",
        "# In a real scenario, you would pass the actual file paths to the Dataset.\n",
        "\n",
        "# As the images are already loaded and processed in `normalized_images`,\n",
        "# we will create a simplified dataset that directly uses this in-memory data.\n",
        "# This is a temporary adaptation for demonstration purposes with the current notebook state.\n",
        "\n",
        "class InMemoryISBIDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for ISBI 2015 challenge data using in-memory data.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dict, transforms=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dict (dict): Dictionary where keys are patient IDs and values are lists of preprocessed image arrays.\n",
        "            transforms (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.data = []\n",
        "        self.patient_ids = []\n",
        "        for patient_id, images in data_dict.items():\n",
        "            for img in images:\n",
        "                self.data.append({'image': img}) # Assuming no masks for this in-memory example\n",
        "                self.patient_ids.append(patient_id)\n",
        "\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Loads and returns a sample from the dataset at the given index.\"\"\"\n",
        "        sample = self.data[idx]\n",
        "        patient_id = self.patient_ids[idx]\n",
        "\n",
        "        if self.transforms:\n",
        "            # Note: Applying transforms directly to numpy arrays in this simplified example.\n",
        "            # For more complex transforms, you might need to adapt this.\n",
        "            sample['image'] = self.transforms(sample['image'])\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "# Create an instance of the custom Dataset using the normalized_images data\n",
        "# In a real scenario, replace InMemoryISBIDataset with ISBIDataset and provide file paths\n",
        "dataset = InMemoryISBIDataset(normalized_images)\n",
        "\n",
        "# Define batch size and number of workers\n",
        "batch_size = 4\n",
        "num_workers = 2 # Adjust based on your system's capabilities\n",
        "\n",
        "# Create an instance of the DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "print(f\"Created DataLoader with batch size: {batch_size}, shuffle: True, num_workers: {num_workers}\")\n",
        "print(f\"Number of batches in DataLoader: {len(dataloader)}\")\n",
        "\n",
        "# Example of iterating through the DataLoader (optional)\n",
        "# for i, batch in enumerate(dataloader):\n",
        "#     print(f\"Batch {i}: Image shape {batch['image'].shape}\")\n",
        "#     if 'mask' in batch:\n",
        "#         print(f\"Batch {i}: Mask shape {batch['mask'].shape}\")\n",
        "#     if i > 2: # Print details for a few batches\n",
        "#         break"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created DataLoader with batch size: 4, shuffle: True, num_workers: 2\n",
            "Number of batches in DataLoader: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dfa21fb"
      },
      "source": [
        "## Integrar aumentaciones de datos\n",
        "\n",
        "### Subtask:\n",
        "Ensure that 3D data augmentation transformations are correctly defined and applied within the `__getitem__` method of the custom Dataset class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4414ef8"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries for 3D data augmentation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b70951f"
      },
      "source": [
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    RandRotate,\n",
        "    RandFlip,\n",
        "    RandGaussianNoise,\n",
        "    RandScaleIntensity,\n",
        "    RandShiftIntensity,\n",
        "    Resize,\n",
        "    OrientationD,\n",
        "    ToTensord,\n",
        ")"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b401c98"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the missing library `monai` using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEwugYEMUXCV",
        "outputId": "e67301be-39d1-47dc-c41b-1b0421d78121"
      },
      "source": [
        "%pip install monai"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: monai in /usr/local/lib/python3.12/dist-packages (1.5.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.12/dist-packages (from monai) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from monai) (2.8.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (2025.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.1->monai) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.1->monai) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57801b22"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `monai` is installed, import the necessary libraries for 3D data augmentation and define the transforms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97a88642"
      },
      "source": [
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    RandRotate,\n",
        "    RandFlip,\n",
        "    RandGaussianNoise,\n",
        "    RandScaleIntensity,\n",
        "    RandShiftIntensity,\n",
        "    Resize,\n",
        "    OrientationD,\n",
        "    ToTensord,\n",
        ")\n",
        "\n",
        "# Define 3D data augmentation transforms\n",
        "augment_transforms = Compose(\n",
        "    [\n",
        "        # Spatial transforms\n",
        "        RandRotate(range_x=np.pi/12, range_y=np.pi/12, range_z=np.pi/12, prob=0.5, keep_size=True),\n",
        "        RandFlip(spatial_axis=0, prob=0.5),\n",
        "        RandFlip(spatial_axis=1, prob=0.5),\n",
        "        RandFlip(spatial_axis=2, prob=0.5),\n",
        "        # Intensity transforms\n",
        "        RandGaussianNoise(prob=0.1, std=0.01),\n",
        "        RandScaleIntensity(factors=0.1, prob=0.5),\n",
        "        RandShiftIntensity(offsets=0.1, prob=0.5),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb3c4be2"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the InMemoryISBIDataset class to accept and apply the defined transforms in the __getitem__ method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJyE9sQbUnOT",
        "outputId": "dc7fd5e2-087e-4583-d85f-a6bd2ca146d4"
      },
      "source": [
        "class InMemoryISBIDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for ISBI 2015 challenge data using in-memory data.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dict, transforms=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dict (dict): Dictionary where keys are patient IDs and values are lists of preprocessed image arrays.\n",
        "            transforms (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.data = []\n",
        "        self.patient_ids = []\n",
        "        for patient_id, images in data_dict.items():\n",
        "            for img in images:\n",
        "                # MONAI transforms expect channel first and then spatial dimensions\n",
        "                # Add channel dimension and ensure it's a PyTorch tensor\n",
        "                img_tensor = torch.from_numpy(np.expand_dims(img, axis=0)).float()\n",
        "                self.data.append({'image': img_tensor}) # Assuming no masks for this in-memory example\n",
        "                self.patient_ids.append(patient_id)\n",
        "\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Loads and returns a sample from the dataset at the given index.\"\"\"\n",
        "        sample = self.data[idx]\n",
        "        patient_id = self.patient_ids[idx]\n",
        "\n",
        "        if self.transforms:\n",
        "            # MONAI transforms work with dictionaries\n",
        "            sample = self.transforms(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "# Create an instance of the custom Dataset using the normalized_images data and the defined transforms\n",
        "dataset = InMemoryISBIDataset(normalized_images, transforms=augment_transforms)\n",
        "\n",
        "# Define batch size and number of workers\n",
        "batch_size = 4\n",
        "num_workers = 2 # Adjust based on your system's capabilities\n",
        "\n",
        "# Create an instance of the DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "print(f\"Created DataLoader with batch size: {batch_size}, shuffle: True, num_workers: {num_workers}\")\n",
        "print(f\"Number of batches in DataLoader: {len(dataloader)}\")\n",
        "\n",
        "# Example of iterating through the DataLoader to demonstrate transformations (optional)\n",
        "# for i, batch in enumerate(dataloader):\n",
        "#     print(f\"Batch {i}: Image shape {batch['image'].shape}\")\n",
        "#     if i > 2: # Print details for a few batches\n",
        "#         break"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created DataLoader with batch size: 4, shuffle: True, num_workers: 2\n",
            "Number of batches in DataLoader: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3018ec01"
      },
      "source": [
        "## Probar el dataloader\n",
        "\n",
        "### Subtask:\n",
        "Crear una instancia del DataLoader y obtener un lote de datos para verificar que funciona correctamente y que las aumentaciones se aplican según lo esperado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7448db0"
      },
      "source": [
        "**Reasoning**:\n",
        "Get a batch of data from the dataloader and print the keys and shapes to verify the data structure and batch size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "194283b1",
        "outputId": "8f6b33df-01ef-4da5-ba34-c0b63b6daf2e"
      },
      "source": [
        "# Get one batch from the DataLoader to verify its structure and the applied transformations\n",
        "try:\n",
        "    data_batch = next(iter(dataloader))\n",
        "\n",
        "    # Print the keys and shapes of the tensors in the batch\n",
        "    print(\"Keys in the batch:\", data_batch.keys())\n",
        "    for key, value in data_batch.items():\n",
        "        print(f\"Shape of '{key}': {value.shape}\")\n",
        "\n",
        "except StopIteration:\n",
        "    print(\"The dataloader is empty.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while getting a batch: {e}\")"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while getting a batch: Caught RuntimeError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/transform.py\", line 150, in apply_transform\n",
            "    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n",
            "    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/spatial/array.py\", line 1414, in __call__\n",
            "    ndim = len(img.peek_pending_shape() if isinstance(img, MetaTensor) else img.shape[1:])\n",
            "                                                                            ^^^^^^^^^\n",
            "AttributeError: 'dict' object has no attribute 'shape'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
            "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"/tmp/ipython-input-3761761811.py\", line 33, in __getitem__\n",
            "    sample = self.transforms(sample)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/compose.py\", line 346, in __call__\n",
            "    result = execute_compose(\n",
            "             ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/compose.py\", line 116, in execute_compose\n",
            "    data = apply_transform(\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/transform.py\", line 180, in apply_transform\n",
            "    raise RuntimeError(f\"applying transform {transform}\") from e\n",
            "RuntimeError: applying transform <monai.transforms.spatial.array.RandRotate object at 0x7c47c4d0c5c0>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f5ea813"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that a MONAI transform is receiving a dictionary instead of the expected image data tensor. MONAI transforms that operate directly on arrays (like `RandRotate`) expect the input to be the array itself, not a dictionary containing the array. The `Compose` transform was expecting a dictionary, but the individual transforms within it are causing the error. We need to adjust how the transforms are applied within the `__getitem__` method or wrap the individual transforms using dictionary-based wrappers provided by MONAI. Wrapping the transforms with dictionary keys is the standard MONAI way to handle dictionary inputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "cf23d427",
        "outputId": "f33842d4-385b-4178-b542-ec213602c423"
      },
      "source": [
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    RandRotate,\n",
        "    RandFlip,\n",
        "    RandGaussianNoise,\n",
        "    RandScaleIntensity,\n",
        "    RandShiftIntensity,\n",
        "    EnsureChannelFirst,\n",
        "    ToTensor,\n",
        "    SpatialCrop, # Assuming we still need cropping/resizing, although done earlier\n",
        "    Resize, # Or resizing\n",
        "    Orientation,\n",
        "    LoadImaged, # To load image data from dictionary keys\n",
        "    SpatialCropd, # Dictionary version of SpatialCrop\n",
        "    Resized, # Dictionary version of Resize\n",
        "    RandRotated, # Dictionary version of RandRotate\n",
        "    RandFlipd, # Dictionary version of RandFlip\n",
        "    RandGaussianNoised, # Dictionary version of RandGaussianNoise\n",
        "    RandScaleIntensityd, # Dictionary version of RandScaleIntensity\n",
        "    RandShiftIntensityd, # Dictionary version of RandShiftIntensity\n",
        "    NormalizeIntensityd # Dictionary version of NormalizeIntensity\n",
        ")\n",
        "\n",
        "from monai.transforms.utility.dictionary import AddChanneld\n",
        "import torch # Import torch\n",
        "import numpy as np # Import numpy if not already\n",
        "from torch.utils.data._utils.collate import default_collate # Import default_collate\n",
        "\n",
        "\n",
        "# Define the transformations using dictionary-based versions\n",
        "# These transforms will be applied within the custom collate_fn\n",
        "augment_transforms = Compose([\n",
        "    # Convert numpy array to PyTorch tensor\n",
        "    ToTensord(keys=['image']),\n",
        "\n",
        "    # 3D Data Augmentation (using dictionary versions)\n",
        "    RandRotated(keys=['image'], range_x=np.pi/12, range_y=np.pi/12, range_z=np.pi/12, prob=0.5, keep_size=True),\n",
        "    RandFlipd(keys=['image'], spatial_axis=[0], prob=0.5),\n",
        "    RandFlipd(keys=['image'], spatial_axis=[1], prob=0.5),\n",
        "    RandFlipd(keys=['image'], spatial_axis=[2], prob=0.5),\n",
        "    RandGaussianNoised(keys=['image'], prob=0.2, std=0.01),\n",
        "    RandScaleIntensityd(keys=['image'], factors=(-0.1, 0.1), prob=0.5),\n",
        "    RandShiftIntensityd(keys=['image'], offsets=(-0.1, 0.1), prob=0.5),\n",
        "\n",
        "    # You might add other transforms here as needed\n",
        "])\n",
        "\n",
        "\n",
        "class InMemoryISBIDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for ISBI 2015 challenge data using in-memory data.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dict): # Removed transforms argument\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dict (dict): Dictionary where keys are patient IDs and values are lists of preprocessed image arrays.\n",
        "        \"\"\"\n",
        "        self.data = []\n",
        "        self.patient_ids = []\n",
        "        for patient_id, images in data_dict.items():\n",
        "            for img in images:\n",
        "                # Store as a dictionary with the numpy array.\n",
        "                # Ensure the numpy array is float32 as expected.\n",
        "                self.data.append({'image': img.astype(np.float32)})\n",
        "                self.patient_ids.append(patient_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Loads and returns a sample from the dataset at the given index.\"\"\"\n",
        "        # Retrieve the sample dictionary containing the numpy array\n",
        "        sample = self.data[idx]\n",
        "        patient_id = self.patient_ids[idx]\n",
        "\n",
        "        # Create a copy of the sample data to avoid modifying the original data in self.data\n",
        "        sample = {key: value.copy() if isinstance(value, np.ndarray) else value for key, value in sample.items()}\n",
        "\n",
        "        # Return the sample dictionary (with numpy array) to the collate_fn\n",
        "        return sample\n",
        "\n",
        "\n",
        "# Define a custom collate function to apply transforms and then batch\n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function to apply MONAI transforms to each sample in a batch\n",
        "    and then use default_collate to batch the results.\n",
        "    \"\"\"\n",
        "    # Apply transforms to each sample in the batch\n",
        "    transformed_batch = [augment_transforms(sample) for sample in batch]\n",
        "\n",
        "    # Use default_collate to combine the transformed samples into a batch\n",
        "    return default_collate(transformed_batch)\n",
        "\n",
        "\n",
        "# Create an instance of the custom Dataset using the normalized_images data\n",
        "dataset = InMemoryISBIDataset(normalized_images) # No transforms passed here\n",
        "\n",
        "# Define batch size and number of workers\n",
        "batch_size = 4\n",
        "num_workers = 2 # Set num_workers back to > 0 to test multiprocessing\n",
        "\n",
        "# Create an instance of the DataLoader with the custom collate_fn\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=custom_collate_fn)\n",
        "\n",
        "print(f\"Created DataLoader with batch size: {batch_size}, shuffle: True, num_workers: {num_workers}, using custom_collate_fn\")\n",
        "print(f\"Number of batches in DataLoader: {len(dataloader)}\")"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'AddChanneld' from 'monai.transforms.utility.dictionary' (/usr/local/lib/python3.12/dist-packages/monai/transforms/utility/dictionary.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2228573666.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmonai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAddChanneld\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;31m# Import torch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m \u001b[0;31m# Import numpy if not already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AddChanneld' from 'monai.transforms.utility.dictionary' (/usr/local/lib/python3.12/dist-packages/monai/transforms/utility/dictionary.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a335aead"
      },
      "source": [
        "## Crear DataLoaders para Train, Val y Test\n",
        "\n",
        "### Subtask:\n",
        "Instanciar DataLoaders separados para los conjuntos de entrenamiento, validación y prueba utilizando las listas de archivos generadas y la clase Dataset personalizada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d29a748"
      },
      "source": [
        "**Reasoning**:\n",
        "Instantiate the `ISBIDatasetFromFiles` class for each split (train, val, test) using the corresponding file lists. Then, create `DataLoader` instances for each dataset, setting `shuffle=True` for the training DataLoader and `shuffle=False` for the validation and test DataLoaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7fd4226",
        "outputId": "dc7595b0-3658-4df5-fc31-103612aae44a"
      },
      "source": [
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    LoadImaged, # Use dictionary version to load from file - Removed from eval_transforms Compose\n",
        "    ToTensord, # Use dictionary version\n",
        "    # Add other necessary transforms here if needed for evaluation like Resize, Orientation, etc.\n",
        ")\n",
        "\n",
        "# from monai.transforms.utility.dictionary import AddChanneld # Removing this import\n",
        "\n",
        "import os # Import os if not already available\n",
        "\n",
        "# Assuming ISBIDatasetFromFiles class and data_transforms are defined in a previous cell\n",
        "# Assuming train_files, val_files, and test_files lists are available from the previous split step\n",
        "\n",
        "# --- Verification: Print sample file paths ---\n",
        "print(\"Sample train_files paths:\")\n",
        "for i in range(min(3, len(train_files))):\n",
        "    print(train_files[i])\n",
        "\n",
        "print(\"\\nSample val_files paths:\")\n",
        "for i in range(min(3, len(val_files))):\n",
        "    print(val_files[i])\n",
        "\n",
        "print(\"\\nSample test_files paths:\")\n",
        "for i in range(min(3, len(test_files))):\n",
        "    print(test_files[i])\n",
        "# --- End Verification ---\n",
        "\n",
        "\n",
        "# Create Dataset instances for each split\n",
        "train_dataset = ISBIDatasetFromFiles(\n",
        "    data=[{\"image\": f, \"patient_id\": os.path.basename(os.path.dirname(f))} for f in train_files],\n",
        "    transforms=data_transforms # Apply transforms (including augmentation) to the training set\n",
        ")\n",
        "\n",
        "val_dataset = ISBIDatasetFromFiles(\n",
        "    data=[{\"image\": f, \"patient_id\": os.path.basename(os.path.dirname(f))} for f in val_files],\n",
        "    transforms=None # Will use eval_transforms below\n",
        ")\n",
        "\n",
        "test_dataset = ISBIDatasetFromFiles(\n",
        "    data=[{\"image\": f, \"patient_id\": os.path.basename(os.path.dirname(f))} for f in test_files],\n",
        "    transforms=None # Will use eval_transforms below\n",
        ")\n",
        "\n",
        "# Define evaluation transforms (without augmentation) - Operates on tensors provided by Dataset\n",
        "eval_transforms = Compose([\n",
        "    # LoadImaged(keys=[\"image\"]), # Loading is done in Dataset.__getitem__ with np.load\n",
        "    # AddChanneld(keys=[\"image\"]), # Channel is added in Dataset.__getitem__\n",
        "    # ToTensord(keys=[\"image\"]), # Conversion to tensor is done in Dataset.__getitem__\n",
        "\n",
        "    # Add any other necessary preprocessing transforms for evaluation here that operate on tensors\n",
        "    # For now, assuming no additional evaluation-specific tensor transforms are needed beyond what's in __getitem__\n",
        "    # If you need transforms like Resize or Orientation for evaluation, add them here using their dictionary versions\n",
        "    # Example: Resized(keys=[\"image\"], spatial_size=target_size)\n",
        "])\n",
        "\n",
        "# Update val and test datasets to use eval_transforms\n",
        "# If eval_transforms is empty, passing None might be clearer, but an empty Compose is also valid.\n",
        "# Let's pass eval_transforms, assuming it might contain transforms in the future.\n",
        "val_dataset.transforms = eval_transforms\n",
        "test_dataset.transforms = eval_transforms\n",
        "\n",
        "\n",
        "# Define batch size and number of workers\n",
        "batch_size = 4\n",
        "num_workers = 2 # Adjust based on your system's capabilities\n",
        "\n",
        "# Create DataLoader instances for each split\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True, # Shuffle training data\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False, # Do not shuffle validation data\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False, # Do not shuffle test data\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "print(f\"Created DataLoaders with batch size: {batch_size}, num_workers: {num_workers}\")\n",
        "print(f\"Number of batches in train_dataloader: {len(train_dataloader)}\")\n",
        "print(f\"Number of batches in val_dataloader: {len(val_dataloader)}\")\n",
        "print(f\"Number of batches in test_dataloader: {len(test_dataloader)}\")\n",
        "\n",
        "# Example of getting a batch from the train_dataloader (optional)\n",
        "# try:\n",
        "#     train_batch = next(iter(train_dataloader))\n",
        "#     print(\"\\nSuccessfully loaded a batch from train_dataloader.\")\n",
        "#     print(\"Keys in the train_batch:\", train_batch.keys())\n",
        "#     for key, value in train_batch.items():\n",
        "#         print(f\"Shape of '{key}': {value.shape}\")\n",
        "# except StopIteration:\n",
        "#     print(\"\\nThe train_dataloader is empty.\")\n",
        "# except Exception as e:\n",
        "#      print(f\"\\nAn error occurred while getting a batch from train_dataloader: {e}\")\n",
        "\n",
        "# Example of getting a batch from the val_dataloader (optional)\n",
        "# try:\n",
        "#     val_batch = next(iter(val_dataloader))\n",
        "#     print(\"\\nSuccessfully loaded a batch from val_dataloader.\")\n",
        "#     print(\"Keys in the val_batch:\", val_batch.keys())\n",
        "#     for key, value in val_batch.items():\n",
        "#         print(f\"Shape of '{key}': {value.shape}\")\n",
        "# except StopIteration:\n",
        "#     print(\"\\nThe val_dataloader is empty.\")\n",
        "# except Exception as e:\n",
        "#      print(f\"\\nAn error occurred while getting a batch from val_dataloader: {e}\")"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample train_files paths:\n",
            "/content/ISBI_preprocessed_data/training01/training01_image_000.npy\n",
            "/content/ISBI_preprocessed_data/training01/training01_image_001.npy\n",
            "/content/ISBI_preprocessed_data/training01/training01_image_002.npy\n",
            "\n",
            "Sample val_files paths:\n",
            "/content/ISBI_preprocessed_data/training05/training05_image_000.npy\n",
            "/content/ISBI_preprocessed_data/training05/training05_image_001.npy\n",
            "/content/ISBI_preprocessed_data/training05/training05_image_002.npy\n",
            "\n",
            "Sample test_files paths:\n",
            "/content/ISBI_preprocessed_data/training02/training02_image_000.npy\n",
            "/content/ISBI_preprocessed_data/training02/training02_image_001.npy\n",
            "/content/ISBI_preprocessed_data/training02/training02_image_002.npy\n",
            "Created DataLoaders with batch size: 4, num_workers: 2\n",
            "Number of batches in train_dataloader: 13\n",
            "Number of batches in val_dataloader: 4\n",
            "Number of batches in test_dataloader: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d9bf211"
      },
      "source": [
        "## 1. Importar bibliotecas necesarias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15701cb7"
      },
      "source": [
        "**Reasoning**:\n",
        "Importar las bibliotecas de PyTorch y MONAI necesarias para construir redes neuronales convolucionales 3D."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7f6f73d"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from monai.networks.nets import BasicUNet, UNet # Importing common MONAI network architectures as examples\n",
        "# from monai.networks.layers import ConvSN # Removed problematic import\n",
        "\n",
        "# You might need other imports depending on the specific CNN architecture you choose"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9813738",
        "outputId": "4a1d08bb-671b-4a1c-dc61-d455e0835b54"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "source_dir = '/content/ISBI_preprocessed_data'\n",
        "destination_dir = '/content/drive/MyDrive/TFM ISBI/semana 3/S3 d1-2'\n",
        "\n",
        "# Create the destination directory in Google Drive if it doesn't exist\n",
        "# os.makedirs will create intermediate directories as well\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# Copy the directory\n",
        "# Use shutil.copytree for copying directories\n",
        "try:\n",
        "    # Check if the source directory exists\n",
        "    if os.path.exists(source_dir):\n",
        "        # Construct the full destination path including the copied folder name\n",
        "        destination_path = os.path.join(destination_dir, os.path.basename(source_dir))\n",
        "        # Copy the entire directory tree\n",
        "        shutil.copytree(source_dir, destination_path)\n",
        "        print(f\"Carpeta '{source_dir}' copiada exitosamente a '{destination_path}'.\")\n",
        "    else:\n",
        "        print(f\"Error: La carpeta de origen '{source_dir}' no existe.\")\n",
        "\n",
        "except FileExistsError:\n",
        "    print(f\"Error: La carpeta de destino '{destination_path}' ya existe.\")\n",
        "    print(\"Por favor, elimina la carpeta existente en Google Drive o especifica una ruta de destino diferente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocurrió un error al copiar la carpeta: {e}\")"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: La carpeta de destino '/content/drive/MyDrive/TFM ISBI/semana 3/S3 d1-2/ISBI_preprocessed_data' ya existe.\n",
            "Por favor, elimina la carpeta existente en Google Drive o especifica una ruta de destino diferente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbff91f5"
      },
      "source": [
        "## 3. Configurar el dispositivo (CPU/GPU)\n",
        "\n",
        "### Subtask:\n",
        "Configurar el dispositivo (GPU si está disponible, de lo contrario CPU) donde se ejecutará el modelo y mover el modelo a este dispositivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b11751f"
      },
      "source": [
        "**Reasoning**:\n",
        "Check for CUDA availability and set the device accordingly. Instantiate the model and move it to the selected device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbce74e9",
        "outputId": "c1eb492c-3d66-44cc-cfb1-2268f704c136"
      },
      "source": [
        "# Check if CUDA is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Instantiate the model (assuming SimpleCNN3DFeatures is defined in a previous cell)\n",
        "# You can adjust in_channels and base_filters as needed\n",
        "model = SimpleCNN3DFeatures(in_channels=1, base_filters=16) # Example instantiation\n",
        "\n",
        "# Move the model to the selected device\n",
        "model.to(device)\n",
        "\n",
        "print(\"Model moved to the selected device.\")\n",
        "\n",
        "# You can optionally print the model architecture to verify\n",
        "# print(model)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Model moved to the selected device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4082f3de"
      },
      "source": [
        "## 2. Definir la arquitectura de la CNN 3D\n",
        "\n",
        "### Subtask:\n",
        "Diseñar la arquitectura de la CNN 3D para extracción de features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "076e7f7f"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a PyTorch `nn.Module` that represents a basic 3D CNN architecture suitable for feature extraction, including convolutional, activation, and pooling layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd33dd46"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleCNN3DFeatures(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple 3D CNN model for feature extraction.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=1, base_filters=32):\n",
        "        super(SimpleCNN3DFeatures, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, base_filters, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(base_filters, base_filters, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv3d(base_filters, base_filters * 2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(base_filters * 2, base_filters * 2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv3d(base_filters * 2, base_filters * 4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(base_filters * 4, base_filters * 4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Add more layers if needed for deeper feature extraction\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the CNN. Returns feature maps.\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        # The output of the last convolutional/pooling layer are the features\n",
        "        return x\n",
        "\n",
        "# Example of how to instantiate the model:\n",
        "# model = SimpleCNN3DFeatures(in_channels=1, base_filters=16)\n",
        "# print(model)\n",
        "\n",
        "# Example of a dummy forward pass (assuming input size is (Batch, Channels, Depth, Height, Width))\n",
        "# dummy_input = torch.randn(1, 1, 128, 128, 128) # Example input shape\n",
        "# features = model(dummy_input)\n",
        "# print(f\"Shape of output features: {features.shape}\")"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62e91836"
      },
      "source": [
        "## Realizar Split Estratificado por Paciente\n",
        "\n",
        "### Subtask:\n",
        "Dividir los datos organizados por paciente en conjuntos de entrenamiento, validación y prueba, manteniendo las imágenes de cada paciente juntas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d6998f6"
      },
      "source": [
        "**Reasoning**:\n",
        "Use `train_test_split` from scikit-learn to split the list of patient IDs. Then, use these lists of patient IDs to gather the corresponding file paths for the training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04450b5f",
        "outputId": "161981c9-9822-4a9c-9108-2f269fb568d3"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np # Import numpy\n",
        "import os # Import os to join paths if necessary\n",
        "\n",
        "# Assuming preprocessed_file_paths dictionary is available from the previous step\n",
        "# and contains paths to the saved .npy files organized by patient.\n",
        "# Example structure: preprocessed_file_paths = {'training01': ['/path/to/training01_image_000.npy', ...], ...}\n",
        "\n",
        "# Get the list of patient IDs from the keys of preprocessed_file_paths\n",
        "patient_ids = list(preprocessed_file_paths.keys())\n",
        "\n",
        "# Define the split ratios\n",
        "train_split_ratio = 0.7\n",
        "val_split_ratio = 0.15\n",
        "test_split_ratio = 0.15\n",
        "\n",
        "# Ensure the ratios sum to 1 (approximately due to floating point)\n",
        "if not np.isclose(train_split_ratio + val_split_ratio + test_split_ratio, 1.0):\n",
        "    raise ValueError(\"Split ratios must sum to 1.0\")\n",
        "\n",
        "# First split: Train and Temp (Val + Test) based on patient IDs\n",
        "train_patients, temp_patients = train_test_split(\n",
        "    patient_ids,\n",
        "    train_size=train_split_ratio,\n",
        "    random_state=42 # for reproducibility\n",
        ")\n",
        "\n",
        "# Calculate the adjusted validation ratio for the second split\n",
        "# val_split_ratio / (val_split_ratio + test_split_ratio)\n",
        "val_split_ratio_adjusted = val_split_ratio / (val_split_ratio + test_split_ratio)\n",
        "\n",
        "# Second split: Val and Test from Temp based on patient IDs\n",
        "val_patients, test_patients = train_test_split(\n",
        "    temp_patients,\n",
        "    train_size=val_split_ratio_adjusted,\n",
        "    random_state=42 # for reproducibility\n",
        ")\n",
        "\n",
        "print(f\"Total patients: {len(patient_ids)}\")\n",
        "print(f\"Train patients: {len(train_patients)}\")\n",
        "print(f\"Validation patients: {len(val_patients)}\")\n",
        "print(f\"Test patients: {len(test_patients)}\")\n",
        "\n",
        "# Now, gather the file paths for each split using the preprocessed_file_paths dictionary\n",
        "train_files = []\n",
        "for patient_id in train_patients:\n",
        "    if patient_id in preprocessed_file_paths:\n",
        "        train_files.extend(preprocessed_file_paths[patient_id])\n",
        "\n",
        "val_files = []\n",
        "for patient_id in val_patients:\n",
        "     if patient_id in preprocessed_file_paths:\n",
        "        val_files.extend(preprocessed_file_paths[patient_id])\n",
        "\n",
        "test_files = []\n",
        "for patient_id in test_patients:\n",
        "     if patient_id in preprocessed_file_paths:\n",
        "        test_files.extend(preprocessed_file_paths[patient_id])\n",
        "\n",
        "\n",
        "print(f\"\\nTotal images: {len(train_files) + len(val_files) + len(test_files)}\")\n",
        "print(f\"Train images: {len(train_files)}\")\n",
        "print(f\"Validation images: {len(val_files)}\")\n",
        "print(f\"Test images: {len(test_files)}\")\n",
        "\n",
        "# You can now use train_files, val_files, and test_files to create your DataLoaders"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total patients: 5\n",
            "Train patients: 3\n",
            "Validation patients: 1\n",
            "Test patients: 1\n",
            "\n",
            "Total images: 84\n",
            "Train images: 52\n",
            "Validation images: 16\n",
            "Test images: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62f2e677"
      },
      "source": [
        "## Organizar datos por paciente\n",
        "\n",
        "### Subtask:\n",
        "Agrupar las rutas de archivo de las imágenes preprocesadas por paciente para facilitar el split estratificado considerando el aspecto longitudinal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c243eea9"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the `preprocessed_file_paths` dictionary, which is already organized by patient directory, and further organize the files within each patient directory, potentially by visit number if derivable from the filename. However, given the filename structure (`trainingXX_image_YYY.npy`), simply listing the files per patient directory is sufficient for now to keep patient data together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24a1fd74",
        "outputId": "8b01ecc1-ba89-4a1f-dbea-e2308994e5bb"
      },
      "source": [
        "import os\n",
        "\n",
        "# preprocessed_file_paths is available from the previous step where we saved the .npy files\n",
        "\n",
        "organized_data = {}\n",
        "\n",
        "for patient_dir, file_paths in preprocessed_file_paths.items():\n",
        "    patient_id = patient_dir # Assuming the directory name is the patient ID\n",
        "    # For simplicity, store all file paths for a patient under their ID\n",
        "    organized_data[patient_id] = file_paths\n",
        "\n",
        "print(\"Datos organizados por paciente:\")\n",
        "for patient_id, files in organized_data.items():\n",
        "    print(f\"Paciente {patient_id}: {len(files)} imágenes preprocesadas\")\n",
        "\n",
        "# You can inspect the organized_data dictionary to see the structure\n",
        "# print(organized_data)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos organizados por paciente:\n",
            "Paciente training03: 20 imágenes preprocesadas\n",
            "Paciente training05: 16 imágenes preprocesadas\n",
            "Paciente training01: 16 imágenes preprocesadas\n",
            "Paciente training04: 16 imágenes preprocesadas\n",
            "Paciente training02: 16 imágenes preprocesadas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bf81d32"
      },
      "source": [
        "## Actualizar Dataset para cargar desde archivos y aplicar transformaciones\n",
        "\n",
        "### Subtask:\n",
        "Modificar la clase Dataset para cargar imágenes desde los archivos `.npy` guardados y aplicar las transformaciones de MONAI, incluyendo aumentación de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e646c72"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a new Dataset class or modify the existing one to accept a list of file paths. Implement the `__getitem__` method to load the image data from the `.npy` file using `np.load` or MONAI's `LoadImaged`, and then apply the MONAI `Compose` of transforms, including `AddChanneld` and the augmentation transforms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "946df6a2",
        "outputId": "08ac465d-f825-428c-a26c-9cc218b15cad"
      },
      "source": [
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    RandRotate,\n",
        "    RandFlip,\n",
        "    RandGaussianNoise,\n",
        "    RandScaleIntensity,\n",
        "    RandShiftIntensity,\n",
        "    EnsureChannelFirstd, # Use dictionary version\n",
        "    ToTensord, # Use dictionary version\n",
        "    LoadImaged, # Use dictionary version to load from file - Removing this from Compose\n",
        "    RandRotated,\n",
        "    RandFlipd,\n",
        "    RandGaussianNoised,\n",
        "    RandScaleIntensityd,\n",
        "    RandShiftIntensityd,\n",
        "    NormalizeIntensityd # If needed, but we already normalized\n",
        ")\n",
        "\n",
        "# from monai.transforms.utility.dictionary import AddChanneld # Removing this as well\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "# Assuming preprocessed_file_paths dictionary is available from the previous step\n",
        "# and contains paths to the saved .npy files organized by patient.\n",
        "\n",
        "# Flatten the list of file paths and create a list of data dictionaries for the Dataset\n",
        "data_dicts = []\n",
        "for patient_id, file_paths in preprocessed_file_paths.items():\n",
        "    for file_path in file_paths:\n",
        "        # Assuming mask files are not being used in this DataLoader for now\n",
        "        data_dicts.append({\"image\": file_path, \"patient_id\": patient_id})\n",
        "\n",
        "\n",
        "# Define the transformations using dictionary-based versions\n",
        "# This Compose will be applied to the dictionary returned by __getitem__\n",
        "# It includes loading, ensuring channel first, converting to tensor, and augmentation\n",
        "data_transforms = Compose([\n",
        "    # Loading with np.load and adding channel will be done in __getitem__\n",
        "\n",
        "    # Convert numpy array to PyTorch tensor - This will be done in __getitem__ now\n",
        "    # ToTensord(keys=[\"image\"]), # Removing this from Compose\n",
        "\n",
        "    # 3D Data Augmentation (using dictionary versions) - These should operate on tensors\n",
        "    RandRotated(keys=['image'], range_x=np.pi/12, range_y=np.pi/12, range_z=np.pi/12, prob=0.5, keep_size=True),\n",
        "    RandFlipd(keys=['image'], spatial_axis=[0], prob=0.5),\n",
        "    RandFlipd(keys=['image'], spatial_axis=[1], prob=0.5),\n",
        "    RandFlipd(keys=['image'], spatial_axis=[2], prob=0.5),\n",
        "    RandGaussianNoised(keys=['image'], prob=0.2, std=0.01),\n",
        "    RandScaleIntensityd(keys=['image'], factors=(-0.1, 0.1), prob=0.5),\n",
        "    RandShiftIntensityd(keys=['image'], offsets=(-0.1, 0.1), prob=0.5),\n",
        "\n",
        "    # You might add other transforms here as needed\n",
        "    # Ensure the final output is a tensor if not guaranteed by augmentation transforms\n",
        "    # ToTensord(keys=[\"image\"]) # Re-adding ToTensord at the end to be sure\n",
        "])\n",
        "\n",
        "\n",
        "class ISBIDatasetFromFiles(Dataset):\n",
        "    \"\"\"Custom Dataset for ISBI 2015 challenge data loading from files.\"\"\"\n",
        "\n",
        "    def __init__(self, data, transforms=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data (list): List of dictionaries, where each dictionary contains file paths (e.g., {\"image\": \"path/to/image.npy\", \"mask\": \"path/to/mask.npy\"}).\n",
        "            transforms (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Loads and returns a sample from the dataset at the given index.\"\"\"\n",
        "        sample_info = self.data[idx]\n",
        "        image_path = sample_info[\"image\"]\n",
        "        patient_id = sample_info[\"patient_id\"]\n",
        "\n",
        "        # Load the image data directly using numpy.load\n",
        "        image_array = np.load(image_path)\n",
        "\n",
        "        # Explicitly convert numpy array to tensor and add channel dimension\n",
        "        # MONAI transforms expect channel first [C, D, H, W]\n",
        "        image_tensor = torch.from_numpy(np.expand_dims(image_array, axis=0)).float()\n",
        "\n",
        "        # Create the sample dictionary with the tensor\n",
        "        sample = {\"image\": image_tensor, \"patient_id\": patient_id}\n",
        "\n",
        "        if self.transforms:\n",
        "            # Apply the Compose of dictionary-based transforms to the sample dictionary\n",
        "            # These transforms should operate on tensors\n",
        "            sample = self.transforms(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "# Create an instance of the custom Dataset using the list of data dictionaries and the defined transforms\n",
        "dataset = ISBIDatasetFromFiles(data_dicts, transforms=data_transforms)\n",
        "\n",
        "# Define batch size and number of workers\n",
        "batch_size = 4\n",
        "num_workers = 2 # Set num_workers > 0 to utilize multiprocessing\n",
        "\n",
        "# Create an instance of the DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "print(f\"Created DataLoader with batch size: {batch_size}, shuffle: True, num_workers: {num_workers}\")\n",
        "print(f\"Number of batches in DataLoader: {len(dataloader)}\")\n",
        "\n",
        "# Example of iterating through the DataLoader to test\n",
        "try:\n",
        "    data_batch = next(iter(dataloader))\n",
        "    print(\"\\nSuccessfully loaded a batch from the DataLoader.\")\n",
        "    print(\"Keys in the batch:\", data_batch.keys())\n",
        "    for key, value in data_batch.items():\n",
        "        print(f\"Shape of '{key}': {value.shape}\")\n",
        "except StopIteration:\n",
        "    print(\"\\nThe dataloader is empty.\")\n",
        "except Exception as e:\n",
        "     print(f\"\\nAn error occurred while getting a batch: {e}\")"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created DataLoader with batch size: 4, shuffle: True, num_workers: 2\n",
            "Number of batches in DataLoader: 21\n",
            "\n",
            "Successfully loaded a batch from the DataLoader.\n",
            "Keys in the batch: dict_keys(['image', 'patient_id'])\n",
            "Shape of 'image': torch.Size([4, 1, 128, 128, 128])\n",
            "\n",
            "An error occurred while getting a batch: 'list' object has no attribute 'shape'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0419866"
      },
      "source": [
        "## Guardar imágenes preprocesadas en disco\n",
        "\n",
        "### Subtask:\n",
        "Guardar los arrays de imágenes preprocesadas (normalizadas y redimensionadas) en archivos `.npy` para su carga eficiente posterior con un `DataLoader`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19f3f8b8"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterar sobre el diccionario de imágenes normalizadas, crear directorios de salida y guardar cada array de imagen como un archivo `.npy`, manteniendo una estructura de carpetas similar a la original pero para los datos preprocesados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dbd0d8a",
        "outputId": "a2d97ac0-47af-4afc-ce4a-f5056ec1b3b4"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Define the output directory for preprocessed images\n",
        "output_base_dir = '/content/ISBI_preprocessed_data'\n",
        "\n",
        "# Create the base output directory if it doesn't exist\n",
        "if not os.path.exists(output_base_dir):\n",
        "    os.makedirs(output_base_dir)\n",
        "\n",
        "preprocessed_file_paths = {}\n",
        "\n",
        "# Iterate over the normalized images dictionary\n",
        "for patient_dir, images in normalized_images.items():\n",
        "    patient_output_dir = os.path.join(output_base_dir, patient_dir)\n",
        "\n",
        "    # Create patient output directory if it doesn't exist\n",
        "    if not os.path.exists(patient_output_dir):\n",
        "        os.makedirs(patient_output_dir)\n",
        "\n",
        "    preprocessed_file_paths[patient_dir] = []\n",
        "\n",
        "    for i, img_array in enumerate(images):\n",
        "        # Define the output file path\n",
        "        # Using patient_dir and index for filename, you might want to use original filename if available\n",
        "        # For simplicity, let's use patient_dir and index for now\n",
        "        output_filename = f\"{patient_dir}_image_{i:03d}.npy\"\n",
        "        output_file_path = os.path.join(patient_output_dir, output_filename)\n",
        "\n",
        "        # Save the numpy array to a .npy file\n",
        "        np.save(output_file_path, img_array)\n",
        "        preprocessed_file_paths[patient_dir].append(output_file_path)\n",
        "\n",
        "        # print(f\"Saved {output_filename}\")\n",
        "\n",
        "print(\"\\nFinished saving preprocessed images to disk.\")\n",
        "\n",
        "# Optional: Display the structure of the saved files\n",
        "print(\"\\nStructure of saved preprocessed data:\")\n",
        "for patient, files in preprocessed_file_paths.items():\n",
        "    print(f\"  {patient}/: {len(files)} files\")\n",
        "    # For brevity, not listing all files here\n",
        "    # for f in files:\n",
        "    #     print(f\"    {os.path.basename(f)}\")"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finished saving preprocessed images to disk.\n",
            "\n",
            "Structure of saved preprocessed data:\n",
            "  training03/: 20 files\n",
            "  training05/: 16 files\n",
            "  training01/: 16 files\n",
            "  training04/: 16 files\n",
            "  training02/: 16 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eb7008b"
      },
      "source": [
        "# Task\n",
        "Implementar un modelo de Deep Learning (CNN 3D ResNet3D-18) para clasificación de fenotipos a partir de imágenes médicas 3D, incluyendo la definición de la arquitectura, función de pérdida, optimizador, bucles de entrenamiento y validación, monitoreo de overfitting y guardado de checkpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59a938f9"
      },
      "source": [
        "## Importar bibliotecas necesarias para el entrenamiento\n",
        "\n",
        "### Subtask:\n",
        "Importar las bibliotecas de PyTorch y MONAI necesarias para definir la arquitectura específica (ResNet3D), la función de pérdida, el optimizador y configurar el bucle de entrenamiento.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98534bd6"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary PyTorch and MONAI libraries for defining the model, loss function, optimizer, and potentially a learning rate scheduler.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d35c351"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from monai.networks.nets import ResNet # Specific import for ResNet\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import StepLR # Example scheduler"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "362d0574"
      },
      "source": [
        "## Implementar la arquitectura base resnet3d-18\n",
        "\n",
        "### Subtask:\n",
        "Cargar o definir la arquitectura ResNet3D-18.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "811d5e94"
      },
      "source": [
        "**Reasoning**:\n",
        "Instantiate the ResNet model with the specified parameters for a 3D ResNet-18 architecture and a placeholder number of output classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "89da339e",
        "outputId": "01a71345-a5f5-4154-8f16-d8461a5fd3fc"
      },
      "source": [
        "# Define the parameters for the ResNet3D-18 model\n",
        "spatial_dims = 3\n",
        "n_input_channels = 1\n",
        "depth = 18\n",
        "num_classes_placeholder = 512  # Placeholder for the number of output features/classes\n",
        "\n",
        "# Instantiate the ResNet model\n",
        "model_resnet = ResNet(\n",
        "    spatial_dims=spatial_dims,\n",
        "    n_input_channels=n_input_channels,\n",
        "    depth=depth,\n",
        "    num_classes=num_classes_placeholder\n",
        ")\n",
        "\n",
        "print(f\"Instantiated ResNet3D-{depth} model with {n_input_channels} input channels and {num_classes_placeholder} placeholder output classes.\")\n",
        "# You can optionally print the model architecture\n",
        "# print(model_resnet)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ResNet.__init__() got an unexpected keyword argument 'depth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2894134429.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Instantiate the ResNet model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m model_resnet = ResNet(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mspatial_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspatial_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mn_input_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_input_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ResNet.__init__() got an unexpected keyword argument 'depth'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0028623"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that the `ResNet` class from MONAI does not accept a `depth` argument directly in its constructor to specify the ResNet-18 architecture. Instead, the architecture variant (like ResNet-18, ResNet-50, etc.) is typically defined by other parameters or by using specific functions or classes provided by MONAI for different ResNet depths. Upon checking the MONAI documentation for `monai.networks.nets.ResNet`, it seems that the `depth` parameter is not a standard argument for this class constructor. The architecture is usually controlled by the configuration of the residual blocks. Let's try instantiating a specific ResNet-18 model from MONAI if available, or adjust the parameters to achieve a similar architecture if a direct 'depth=18' is not supported in this version of MONAI. Looking at the MONAI documentation, there is no direct `ResNet(depth=18)` parameter. The `ResNet` class takes `block`, `layers`, `block_data`, etc. to define the architecture. Alternatively, MONAI provides specific ResNet variants like `resnet10`, `resnet18`, `resnet34`, etc. in `monai.networks.nets`. Let's try importing and using `resnet18`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV2q1eWTqlLZ",
        "outputId": "916fca1a-0ae2-4a1f-d8b6-bdff2d47beb9"
      },
      "source": [
        "# Import the specific resnet18 model from MONAI\n",
        "from monai.networks.nets import resnet18\n",
        "\n",
        "# Define the parameters for the resnet18 model\n",
        "spatial_dims = 3\n",
        "n_input_channels = 1\n",
        "num_classes_placeholder = 512  # Placeholder for the number of output features/classes\n",
        "\n",
        "# Instantiate the resnet18 model\n",
        "model_resnet = resnet18(\n",
        "    spatial_dims=spatial_dims,\n",
        "    n_input_channels=n_input_channels,\n",
        "    num_classes=num_classes_placeholder\n",
        ")\n",
        "\n",
        "print(f\"Instantiated resnet18 model with {n_input_channels} input channels and {num_classes_placeholder} placeholder output classes.\")\n",
        "# You can optionally print the model architecture\n",
        "# print(model_resnet)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instantiated resnet18 model with 1 input channels and 512 placeholder output classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be2e54c7"
      },
      "source": [
        "## Considerar transfer learning (opcional)\n",
        "\n",
        "### Subtask:\n",
        "If pre-trained weights are available for ResNet3D-18, load these weights for transfer learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23fdfc48"
      },
      "source": [
        "**Reasoning**:\n",
        "Check for the existence of a pre-trained model checkpoint file, load it if it exists, and then load the state dictionary into the instantiated ResNet model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e52317f",
        "outputId": "2d9e459c-a4fd-456a-89d6-6f14937d006f"
      },
      "source": [
        "import os # Import os if not already imported\n",
        "\n",
        "# Define the path to the pre-trained checkpoint file\n",
        "# NOTE: Replace 'path/to/your/pretrained_model.pth' with the actual path if you have one\n",
        "pretrained_checkpoint_path = '/content/drive/MyDrive/TFM ISBI/semana 3/S3 d1-2/resnet18_pretrained.pth' # Example path\n",
        "\n",
        "# Check if the checkpoint file exists\n",
        "if os.path.exists(pretrained_checkpoint_path):\n",
        "    print(f\"Pre-trained checkpoint found at: {pretrained_checkpoint_path}\")\n",
        "    try:\n",
        "        # Load the state dictionary from the checkpoint file\n",
        "        # Map to CPU if training on CPU, otherwise remove map_location\n",
        "        state_dict = torch.load(pretrained_checkpoint_path, map_location=device)\n",
        "\n",
        "        # Load the state dictionary into the model\n",
        "        # Use strict=False to allow loading even if some keys don't match (e.g., the final layer)\n",
        "        model_resnet.load_state_dict(state_dict, strict=False)\n",
        "        print(\"Successfully loaded pre-trained weights into the model.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading pre-trained weights: {e}\")\n",
        "        print(\"Proceeding without pre-trained weights.\")\n",
        "else:\n",
        "    print(f\"No pre-trained checkpoint found at: {pretrained_checkpoint_path}\")\n",
        "    print(\"Proceeding with randomly initialized weights.\")\n",
        "\n",
        "# Ensure the model is on the correct device after loading weights\n",
        "model_resnet.to(device)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No pre-trained checkpoint found at: /content/drive/MyDrive/TFM ISBI/semana 3/S3 d1-2/resnet18_pretrained.pth\n",
            "Proceeding with randomly initialized weights.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv3d(1, 64, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), bias=False)\n",
              "  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (act): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): ResNetBlock(\n",
              "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act): ReLU(inplace=True)\n",
              "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): ResNetBlock(\n",
              "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act): ReLU(inplace=True)\n",
              "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): ResNetBlock(\n",
              "      (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
              "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act): ReLU(inplace=True)\n",
              "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): ResNetBlock(\n",
              "      (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act): ReLU(inplace=True)\n",
              "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): ResNetBlock(\n",
              "      (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
              "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act): ReLU(inplace=True)\n",
              "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): ResNetBlock(\n",
              "      (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act): ReLU(inplace=True)\n",
              "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): ResNetBlock(\n",
              "      (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
              "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act): ReLU(inplace=True)\n",
              "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): ResNetBlock(\n",
              "      (conv1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act): ReLU(inplace=True)\n",
              "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92b865c6"
      },
      "source": [
        "## Adaptar la última capa para clasificación de fenotipos\n",
        "\n",
        "### Subtask:\n",
        "Modificar la capa de salida final de ResNet3D-18 para que se ajuste al número de clases de fenotipos en tu dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d2e7809"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the final fully connected layer of the ResNet3D-18 model to match the number of phenotype classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28df68c3",
        "outputId": "9cce813b-02a1-4179-a5ac-06ee860757e5"
      },
      "source": [
        "# Determine the number of unique phenotype classes in your dataset.\n",
        "# This information is not directly available from the current notebook state\n",
        "# (e.g., no dataframe with phenotype labels).\n",
        "# For demonstration purposes, let's assume the number of classes is 5 for now.\n",
        "# You would replace this with the actual number of unique phenotypes from your data labels.\n",
        "num_phenotype_classes = 5 # Replace with the actual number of phenotype classes\n",
        "\n",
        "# Identify the name of the final fully connected layer in the model_resnet architecture.\n",
        "# By inspecting the MONAI ResNet source or printing the model, we know the final layer is 'fc'.\n",
        "\n",
        "# Get the number of input features to the existing final layer.\n",
        "# The 'fc' layer is a torch.nn.Linear layer, and its input features are stored in in_features.\n",
        "num_input_features = model_resnet.fc.in_features\n",
        "\n",
        "# Create a new torch.nn.Linear layer with the correct number of input and output features.\n",
        "new_final_layer = nn.Linear(num_input_features, num_phenotype_classes)\n",
        "\n",
        "# Replace the original final layer with the new layer.\n",
        "model_resnet.fc = new_final_layer\n",
        "\n",
        "# Move the modified model to the configured device.\n",
        "model_resnet.to(device)\n",
        "\n",
        "print(f\"Modified the final layer of ResNet3D-18 to have {num_phenotype_classes} output classes.\")\n",
        "print(\"Model moved to the selected device.\")\n",
        "\n",
        "# Optional: Print the modified model architecture to verify the change\n",
        "# print(model_resnet)\n"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modified the final layer of ResNet3D-18 to have 5 output classes.\n",
            "Model moved to the selected device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97628e62"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the loss function for the classification task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "762c19c8",
        "outputId": "a62705e3-975b-4bb5-e844-8a403b94cd88"
      },
      "source": [
        "# Define the loss function\n",
        "# For a classification task, CrossEntropyLoss is commonly used.\n",
        "# Assuming the task is multi-class classification of phenotypes.\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "print(f\"Defined the loss function: {type(criterion).__name__}\")"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined the loss function: CrossEntropyLoss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0ff4bb4"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the optimizer for training the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44386d53",
        "outputId": "93eb51be-341f-42dc-f7ac-870007d43b25"
      },
      "source": [
        "# Define the optimizer\n",
        "# AdamW is a common and effective optimizer for deep learning models.\n",
        "# We pass the model's parameters to the optimizer.\n",
        "# You can adjust the learning rate (lr) and weight_decay as needed.\n",
        "optimizer = AdamW(model_resnet.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "print(f\"Defined the optimizer: {type(optimizer).__name__}\")\n",
        "print(f\"Optimizer learning rate: {optimizer.defaults['lr']}\")\n",
        "print(f\"Optimizer weight decay: {optimizer.defaults['weight_decay']}\")"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined the optimizer: AdamW\n",
            "Optimizer learning rate: 0.0001\n",
            "Optimizer weight decay: 1e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2a1c121"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the training loop, including iterating through the training data, performing forward and backward passes, and updating model weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d8a59e7",
        "outputId": "08081b82-27e9-47c0-e546-5ddc9f8fb735"
      },
      "source": [
        "import time\n",
        "\n",
        "# Define the number of training epochs\n",
        "num_epochs = 10 # You can adjust this based on your needs\n",
        "\n",
        "# Lists to store loss values for plotting/monitoring\n",
        "train_loss_history = []\n",
        "\n",
        "print(\"Starting training loop...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    model_resnet.train() # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Iterate over the training DataLoader\n",
        "    for batch_idx, batch_data in enumerate(train_dataloader):\n",
        "        # Move data to the appropriate device\n",
        "        inputs = batch_data[\"image\"].to(device)\n",
        "        # Assuming 'phenotype_label' is the key for your phenotype labels\n",
        "        # You might need to adjust this key based on your data dictionary structure\n",
        "        # Assuming phenotype_label is a tensor of class indices (long type)\n",
        "        # NOTE: The current data_dicts and ISBIDatasetFromFiles do not include phenotype labels.\n",
        "        # You will need to add phenotype labels to your data loading process.\n",
        "        # For now, we will use dummy labels to make the code runnable.\n",
        "        # In a real scenario, replace this with actual label loading.\n",
        "        # Placeholder for actual labels: labels = batch_data[\"phenotype_label\"].to(device)\n",
        "\n",
        "        # Create dummy labels based on patient_id for demonstration\n",
        "        # In a real scenario, map patient_id to actual phenotype class index\n",
        "        dummy_labels = torch.randint(0, num_phenotype_classes, (inputs.size(0),)).to(device)\n",
        "        labels = dummy_labels # Replace with actual labels in a real application\n",
        "\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_resnet(inputs)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the loss\n",
        "        running_loss += loss.item() * inputs.size(0) # Accumulate weighted by batch size\n",
        "\n",
        "        # Optional: Print loss every few batches to monitor progress\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "    # Calculate average epoch loss\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    train_loss_history.append(epoch_loss)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] finished. Average Training Loss: {epoch_loss:.4f}, Duration: {epoch_duration:.2f} seconds\")\n",
        "\n",
        "    # Validation loop will be added in the next step\n",
        "    # Checkpointing and early stopping will be added later\n",
        "\n",
        "print(\"\\nTraining finished.\")"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training loop...\n",
            "Epoch [1/10], Batch [10/13], Loss: 1.8204\n",
            "Epoch [1/10] finished. Average Training Loss: 2.1707, Duration: 138.93 seconds\n",
            "Epoch [2/10], Batch [10/13], Loss: 1.7351\n",
            "Epoch [2/10] finished. Average Training Loss: 1.5949, Duration: 139.90 seconds\n",
            "Epoch [3/10], Batch [10/13], Loss: 2.2149\n",
            "Epoch [3/10] finished. Average Training Loss: 1.8251, Duration: 138.35 seconds\n",
            "Epoch [4/10], Batch [10/13], Loss: 1.6211\n",
            "Epoch [4/10] finished. Average Training Loss: 1.7340, Duration: 137.80 seconds\n",
            "Epoch [5/10], Batch [10/13], Loss: 1.7363\n",
            "Epoch [5/10] finished. Average Training Loss: 1.6772, Duration: 138.72 seconds\n",
            "Epoch [6/10], Batch [10/13], Loss: 1.4627\n",
            "Epoch [6/10] finished. Average Training Loss: 1.6241, Duration: 138.75 seconds\n",
            "Epoch [7/10], Batch [10/13], Loss: 1.6552\n",
            "Epoch [7/10] finished. Average Training Loss: 1.7027, Duration: 139.94 seconds\n",
            "Epoch [8/10], Batch [10/13], Loss: 1.7143\n",
            "Epoch [8/10] finished. Average Training Loss: 1.6741, Duration: 140.41 seconds\n",
            "Epoch [9/10], Batch [10/13], Loss: 1.3894\n",
            "Epoch [9/10] finished. Average Training Loss: 1.6382, Duration: 142.86 seconds\n",
            "Epoch [10/10], Batch [10/13], Loss: 1.5866\n",
            "Epoch [10/10] finished. Average Training Loss: 1.6505, Duration: 141.94 seconds\n",
            "\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28d2d5d7"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the validation loop to evaluate the model's performance on the validation set after each training epoch and monitor for overfitting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59c0d2c3",
        "outputId": "68fd1a91-313a-42cd-f8ff-383340ac54e3"
      },
      "source": [
        "# Lists to store validation loss and metrics for monitoring\n",
        "val_loss_history = []\n",
        "# You might also want to track accuracy or other relevant metrics\n",
        "\n",
        "print(\"Starting validation loop...\")\n",
        "\n",
        "for epoch in range(num_epochs): # Iterate through the same epochs as training\n",
        "    model_resnet.eval() # Set the model to evaluation mode\n",
        "    running_val_loss = 0.0\n",
        "    # Initialize metrics if you are tracking them (e.g., correct predictions, total samples)\n",
        "    # correct_predictions = 0\n",
        "    # total_samples = 0\n",
        "\n",
        "    # Disable gradient calculation for validation\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the validation DataLoader\n",
        "        for batch_data in val_dataloader:\n",
        "            # Move data to the appropriate device\n",
        "            inputs = batch_data[\"image\"].to(device)\n",
        "            # Assuming 'phenotype_label' is the key for your phenotype labels\n",
        "            # Use the same dummy labels as in the training loop for consistency with the example\n",
        "            dummy_labels = torch.randint(0, num_phenotype_classes, (inputs.size(0),)).to(device)\n",
        "            labels = dummy_labels # Replace with actual labels in a real application\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model_resnet(inputs)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Accumulate the validation loss\n",
        "            running_val_loss += loss.item() * inputs.size(0) # Accumulate weighted by batch size\n",
        "\n",
        "            # Calculate metrics if tracking (e.g., accuracy)\n",
        "            # _, predicted = torch.max(outputs.data, 1)\n",
        "            # total_samples += labels.size(0)\n",
        "            # correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "    # Calculate average epoch validation loss\n",
        "    epoch_val_loss = running_val_loss / len(val_dataset)\n",
        "    val_loss_history.append(epoch_val_loss)\n",
        "\n",
        "    # Calculate and print metrics if tracking\n",
        "    # epoch_accuracy = correct_predictions / total_samples\n",
        "    # print(f\"Epoch [{epoch+1}/{num_epochs}] - Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Validation Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "    # You can add logic here for monitoring overfitting (e.g., comparing train and val loss)\n",
        "    # and for early stopping or saving checkpoints based on validation performance.\n",
        "\n",
        "\n",
        "print(\"\\nValidation loop finished for all epochs.\")"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting validation loop...\n",
            "Epoch [1/10] - Validation Loss: 3.4477\n",
            "Epoch [2/10] - Validation Loss: 2.9653\n",
            "Epoch [3/10] - Validation Loss: 3.0813\n",
            "Epoch [4/10] - Validation Loss: 2.1537\n",
            "Epoch [5/10] - Validation Loss: 2.7460\n",
            "Epoch [6/10] - Validation Loss: 2.0941\n",
            "Epoch [7/10] - Validation Loss: 2.4481\n",
            "Epoch [8/10] - Validation Loss: 1.7983\n",
            "Epoch [9/10] - Validation Loss: 2.4644\n",
            "Epoch [10/10] - Validation Loss: 2.3939\n",
            "\n",
            "Validation loop finished for all epochs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cae5294"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement saving model checkpoints based on validation performance to allow resuming training or using the best model later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df158ebb",
        "outputId": "c68c90c3-73c5-46d8-d43c-37bbc9bd7b9d"
      },
      "source": [
        "# Initialize variables for tracking the best validation loss and saving the best model\n",
        "best_val_loss = float('inf')\n",
        "best_epoch = -1\n",
        "checkpoint_dir = '/content/checkpoints' # Directory to save checkpoints\n",
        "\n",
        "# Create the checkpoint directory if it doesn't exist\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "print(\"Starting training and validation with checkpointing...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    model_resnet.train() # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Training loop\n",
        "    for batch_idx, batch_data in enumerate(train_dataloader):\n",
        "        inputs = batch_data[\"image\"].to(device)\n",
        "        # Using dummy labels as before\n",
        "        dummy_labels = torch.randint(0, num_phenotype_classes, (inputs.size(0),)).to(device)\n",
        "        labels = dummy_labels\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_resnet(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    train_loss_history.append(epoch_loss)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] finished. Average Training Loss: {epoch_loss:.4f}, Duration: {epoch_duration:.2f} seconds\")\n",
        "\n",
        "    # Validation loop\n",
        "    model_resnet.eval() # Set the model to evaluation mode\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_data in val_dataloader:\n",
        "            inputs = batch_data[\"image\"].to(device)\n",
        "            # Using dummy labels as before\n",
        "            dummy_labels = torch.randint(0, num_phenotype_classes, (inputs.size(0),)).to(device)\n",
        "            labels = dummy_labels\n",
        "\n",
        "            outputs = model_resnet(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_val_loss = running_val_loss / len(val_dataset)\n",
        "    val_loss_history.append(epoch_val_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Validation Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "    # Checkpointing: Save the model if the validation loss has improved\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_epoch = epoch\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f'best_model_epoch_{epoch+1}.pth')\n",
        "        torch.save(model_resnet.state_dict(), checkpoint_path)\n",
        "        print(f\"Validation loss improved. Saving model checkpoint to: {checkpoint_path}\")\n",
        "\n",
        "    # Optional: Implement early stopping based on validation loss\n",
        "    # For example, stop if validation loss hasn't improved for a certain number of epochs\n",
        "\n",
        "print(\"\\nTraining finished.\")\n",
        "print(f\"Best validation loss ({best_val_loss:.4f}) achieved at epoch {best_epoch+1}.\")"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training and validation with checkpointing...\n",
            "Epoch [1/10] finished. Average Training Loss: 1.6687, Duration: 138.85 seconds\n",
            "Epoch [1/10] - Validation Loss: 3.7935\n",
            "Validation loss improved. Saving model checkpoint to: /content/checkpoints/best_model_epoch_1.pth\n",
            "Epoch [2/10] finished. Average Training Loss: 1.6539, Duration: 138.85 seconds\n",
            "Epoch [2/10] - Validation Loss: 3.0709\n",
            "Validation loss improved. Saving model checkpoint to: /content/checkpoints/best_model_epoch_2.pth\n",
            "Epoch [3/10] finished. Average Training Loss: 1.6383, Duration: 141.43 seconds\n",
            "Epoch [3/10] - Validation Loss: 3.1846\n",
            "Epoch [4/10] finished. Average Training Loss: 1.6400, Duration: 139.47 seconds\n",
            "Epoch [4/10] - Validation Loss: 4.2759\n",
            "Epoch [5/10] finished. Average Training Loss: 1.6646, Duration: 140.17 seconds\n",
            "Epoch [5/10] - Validation Loss: 2.4713\n",
            "Validation loss improved. Saving model checkpoint to: /content/checkpoints/best_model_epoch_5.pth\n",
            "Epoch [6/10] finished. Average Training Loss: 1.6457, Duration: 138.44 seconds\n",
            "Epoch [6/10] - Validation Loss: 2.6118\n",
            "Epoch [7/10] finished. Average Training Loss: 1.6548, Duration: 138.56 seconds\n",
            "Epoch [7/10] - Validation Loss: 2.8499\n",
            "Epoch [8/10] finished. Average Training Loss: 1.6530, Duration: 142.04 seconds\n",
            "Epoch [8/10] - Validation Loss: 2.3819\n",
            "Validation loss improved. Saving model checkpoint to: /content/checkpoints/best_model_epoch_8.pth\n",
            "Epoch [9/10] finished. Average Training Loss: 1.6335, Duration: 138.41 seconds\n",
            "Epoch [9/10] - Validation Loss: 2.1965\n",
            "Validation loss improved. Saving model checkpoint to: /content/checkpoints/best_model_epoch_9.pth\n",
            "Epoch [10/10] finished. Average Training Loss: 1.6107, Duration: 139.22 seconds\n",
            "Epoch [10/10] - Validation Loss: 2.5843\n",
            "\n",
            "Training finished.\n",
            "Best validation loss (2.1965) achieved at epoch 9.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d7cbff1"
      },
      "source": [
        "## Implementar learning rate scheduling (opcional)\n",
        "\n",
        "### Subtask:\n",
        "Implement a strategy to adjust the learning rate during training, such as reducing it based on validation metrics or using a predefined schedule.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7050ec44"
      },
      "source": [
        "**Reasoning**:\n",
        "Instantiate a learning rate scheduler (`StepLR`) and update the training loop to use the scheduler's step method after each epoch. Print the learning rate to monitor its value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15c35bc6",
        "outputId": "cf31506d-1979-4d31-ebf1-d785b0e5a415"
      },
      "source": [
        "# Instantiate the learning rate scheduler\n",
        "# StepLR reduces the learning rate by a factor of gamma every step_size epochs.\n",
        "# Adjust step_size and gamma based on your desired schedule.\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.5) # Example: Reduce LR by half every 5 epochs\n",
        "\n",
        "print(f\"Defined the learning rate scheduler: {type(scheduler).__name__}\")\n",
        "print(f\"Initial learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "\n",
        "\n",
        "# Lists to store loss values for plotting/monitoring (already initialized, but showing here for context)\n",
        "# train_loss_history = []\n",
        "# val_loss_history = []\n",
        "\n",
        "# Initialize variables for tracking the best validation loss and saving the best model (already initialized)\n",
        "# best_val_loss = float('inf')\n",
        "# best_epoch = -1\n",
        "# checkpoint_dir = '/content/checkpoints' # Directory to save checkpoints\n",
        "\n",
        "# Create the checkpoint directory if it doesn't exist (already done)\n",
        "# if not os.path.exists(checkpoint_dir):\n",
        "#     os.makedirs(checkpoint_dir)\n",
        "\n",
        "print(\"Starting training and validation with checkpointing and learning rate scheduling...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}] - Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    model_resnet.train() # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Training loop\n",
        "    for batch_idx, batch_data in enumerate(train_dataloader):\n",
        "        inputs = batch_data[\"image\"].to(device)\n",
        "        # Using dummy labels as before\n",
        "        dummy_labels = torch.randint(0, num_phenotype_classes, (inputs.size(0),)).to(device)\n",
        "        labels = dummy_labels\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_resnet(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    train_loss_history.append(epoch_loss)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] finished. Average Training Loss: {epoch_loss:.4f}, Duration: {epoch_duration:.2f} seconds\")\n",
        "\n",
        "    # Validation loop\n",
        "    model_resnet.eval() # Set the model to evaluation mode\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_data in val_dataloader:\n",
        "            inputs = batch_data[\"image\"].to(device)\n",
        "            # Using dummy labels as before\n",
        "            dummy_labels = torch.randint(0, num_phenotype_classes, (inputs.size(0),)).to(device)\n",
        "            labels = dummy_labels\n",
        "\n",
        "            outputs = model_resnet(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_val_loss = running_val_loss / len(val_dataset)\n",
        "    val_loss_history.append(epoch_val_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Validation Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Checkpointing: Save the model if the validation loss has improved\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_epoch = epoch\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f'best_model_epoch_{epoch+1}.pth')\n",
        "        torch.save(model_resnet.state_dict(), checkpoint_path)\n",
        "        print(f\"Validation loss improved. Saving model checkpoint to: {checkpoint_path}\")\n",
        "\n",
        "    # Optional: Implement early stopping based on validation loss\n",
        "    # For example, stop if validation loss hasn't improved for a certain number of epochs\n",
        "\n",
        "print(\"\\nTraining finished.\")\n",
        "print(f\"Best validation loss ({best_val_loss:.4f}) achieved at epoch {best_epoch+1}.\")\n"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined the learning rate scheduler: StepLR\n",
            "Initial learning rate: 0.0001\n",
            "Starting training and validation with checkpointing and learning rate scheduling...\n",
            "\n",
            "Epoch [1/10] - Learning Rate: 0.000100\n",
            "Epoch [1/10] finished. Average Training Loss: 1.6441, Duration: 141.17 seconds\n",
            "Epoch [1/10] - Validation Loss: 2.9057\n",
            "\n",
            "Epoch [2/10] - Learning Rate: 0.000100\n",
            "Epoch [2/10] finished. Average Training Loss: 1.6392, Duration: 137.49 seconds\n",
            "Epoch [2/10] - Validation Loss: 3.3181\n",
            "\n",
            "Epoch [3/10] - Learning Rate: 0.000100\n",
            "Epoch [3/10] finished. Average Training Loss: 1.6390, Duration: 139.55 seconds\n",
            "Epoch [3/10] - Validation Loss: 1.9966\n",
            "Validation loss improved. Saving model checkpoint to: /content/checkpoints/best_model_epoch_3.pth\n",
            "\n",
            "Epoch [4/10] - Learning Rate: 0.000100\n",
            "Epoch [4/10] finished. Average Training Loss: 1.6189, Duration: 141.31 seconds\n",
            "Epoch [4/10] - Validation Loss: 1.4870\n",
            "Validation loss improved. Saving model checkpoint to: /content/checkpoints/best_model_epoch_4.pth\n",
            "\n",
            "Epoch [5/10] - Learning Rate: 0.000100\n",
            "Epoch [5/10] finished. Average Training Loss: 1.6450, Duration: 138.56 seconds\n",
            "Epoch [5/10] - Validation Loss: 1.7137\n",
            "\n",
            "Epoch [6/10] - Learning Rate: 0.000050\n",
            "Epoch [6/10] finished. Average Training Loss: 1.6306, Duration: 138.09 seconds\n",
            "Epoch [6/10] - Validation Loss: 1.6006\n",
            "\n",
            "Epoch [7/10] - Learning Rate: 0.000050\n",
            "Epoch [7/10] finished. Average Training Loss: 1.6210, Duration: 141.39 seconds\n",
            "Epoch [7/10] - Validation Loss: 2.1042\n",
            "\n",
            "Epoch [8/10] - Learning Rate: 0.000050\n",
            "Epoch [8/10] finished. Average Training Loss: 1.6413, Duration: 140.10 seconds\n",
            "Epoch [8/10] - Validation Loss: 1.6186\n",
            "\n",
            "Epoch [9/10] - Learning Rate: 0.000050\n",
            "Epoch [9/10] finished. Average Training Loss: 1.6209, Duration: 143.50 seconds\n",
            "Epoch [9/10] - Validation Loss: 1.5765\n",
            "\n",
            "Epoch [10/10] - Learning Rate: 0.000050\n",
            "Epoch [10/10] finished. Average Training Loss: 1.5646, Duration: 141.03 seconds\n",
            "Epoch [10/10] - Validation Loss: 1.5364\n",
            "\n",
            "Training finished.\n",
            "Best validation loss (1.4870) achieved at epoch 4.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85d9bc5a"
      },
      "source": [
        "## Preparar métricas de evaluación\n",
        "\n",
        "### Subtask:\n",
        "Definir métricas para evaluar el rendimiento del modelo durante el entrenamiento y la validación (ej: Accuracy, F1-score, etc.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a7ebfd9"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary metrics from `monai.metrics` for evaluating the model performance during training and validation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cba55cf",
        "outputId": "6ab0ff57-370f-4f4d-870f-3bdfab6e7d25"
      },
      "source": [
        "from monai.metrics import DiceMetric, MeanIoU # Examples of segmentation metrics, not suitable for classification\n",
        "\n",
        "# For classification, we need classification metrics.\n",
        "# MONAI provides some classification metrics, or we can use scikit-learn.\n",
        "# Let's use scikit-learn for common classification metrics like Accuracy and F1-score.\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "print(\"Imported classification metrics from scikit-learn.\")"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported classification metrics from scikit-learn.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bc88324"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize the chosen metrics before the training loop to accumulate results over epochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bcb5127",
        "outputId": "a89687cd-f201-435d-cbc1-4ad83984306a"
      },
      "source": [
        "# Initialize lists to store metrics per epoch\n",
        "train_accuracy_history = []\n",
        "val_accuracy_history = []\n",
        "val_f1_history = []\n",
        "val_precision_history = []\n",
        "val_recall_history = []\n",
        "\n",
        "print(\"Initialized metric history lists.\")"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized metric history lists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e14629ac"
      },
      "source": [
        "**Reasoning**:\n",
        "Integrate metric calculation into the validation loop, calculating and accumulating metrics per batch, and then averaging for the epoch. Also, calculate training metrics within the training loop for monitoring overfitting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99e7d74b",
        "outputId": "e4023e93-f58d-46e4-e152-83d7b579f1cc"
      },
      "source": [
        "import time # Import time if not already imported\n",
        "\n",
        "# Initialize variables for tracking the best validation metric (e.g., F1-score) and saving the best model\n",
        "best_val_f1 = -1.0 # Initialize with a value lower than any possible F1-score\n",
        "best_epoch = -1\n",
        "checkpoint_dir = '/content/checkpoints' # Directory to save checkpoints (already defined)\n",
        "\n",
        "# Create the checkpoint directory if it doesn't exist (already done)\n",
        "# if not os.path.exists(checkpoint_dir):\n",
        "#     os.makedirs(checkpoint_dir)\n",
        "\n",
        "print(\"Starting training and validation with checkpointing and metric calculation...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}] - Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    # Training loop\n",
        "    model_resnet.train() # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    train_preds = []\n",
        "    train_labels = []\n",
        "\n",
        "    for batch_idx, batch_data in enumerate(train_dataloader):\n",
        "        inputs = batch_data[\"image\"].to(device)\n",
        "        # Using dummy labels as before - In a real scenario, load actual labels\n",
        "        dummy_labels = torch.randint(0, num_phenotype_classes, (inputs.size(0),)).to(device)\n",
        "        labels = dummy_labels\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_resnet(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        # Store predictions and labels for metric calculation\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_preds.extend(predicted.cpu().numpy())\n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    train_loss_history.append(epoch_loss)\n",
        "\n",
        "    # Calculate training metrics\n",
        "    train_accuracy = accuracy_score(train_labels, train_preds)\n",
        "    # You can calculate other training metrics here if needed\n",
        "\n",
        "    train_accuracy_history.append(train_accuracy)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] finished. Average Training Loss: {epoch_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Duration: {epoch_duration:.2f} seconds\")\n",
        "\n",
        "\n",
        "    # Validation loop\n",
        "    model_resnet.eval() # Set the model to evaluation mode\n",
        "    running_val_loss = 0.0\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data in val_dataloader:\n",
        "            inputs = batch_data[\"image\"].to(device)\n",
        "            # Using dummy labels as before - In a real scenario, load actual labels\n",
        "            dummy_labels = torch.randint(0, num_phenotype_classes, (inputs.size(0),)).to(device)\n",
        "            labels = dummy_labels\n",
        "\n",
        "            outputs = model_resnet(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Store predictions and labels for metric calculation\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_preds.extend(predicted.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "    epoch_val_loss = running_val_loss / len(val_dataset)\n",
        "    val_loss_history.append(epoch_val_loss)\n",
        "\n",
        "    # Calculate validation metrics\n",
        "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "    val_f1 = f1_score(val_labels, val_preds, average='weighted') # Use weighted average for multiclass\n",
        "    val_precision = precision_score(val_labels, val_preds, average='weighted')\n",
        "    val_recall = recall_score(val_labels, val_preds, average='weighted')\n",
        "\n",
        "    val_accuracy_history.append(val_accuracy)\n",
        "    val_f1_history.append(val_f1)\n",
        "    val_precision_history.append(val_precision)\n",
        "    val_recall_history.append(val_recall)\n",
        "\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation F1-score: {val_f1:.4f}\")\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Checkpointing: Save the model if the validation F1-score has improved\n",
        "    # Using F1-score as the metric to track for saving the best model\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        best_epoch = epoch\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f'best_model_epoch_{epoch+1}_f1_{best_val_f1:.4f}.pth')\n",
        "        torch.save(model_resnet.state_dict(), checkpoint_path)\n",
        "        print(f\"Validation F1-score improved. Saving model checkpoint to: {checkpoint_path}\")\n",
        "\n",
        "    # Optional: Implement early stopping based on validation metric\n",
        "\n",
        "print(\"\\nTraining finished.\")\n",
        "print(f\"Best validation F1-score ({best_val_f1:.4f}) achieved at epoch {best_epoch+1}.\")"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training and validation with checkpointing and metric calculation...\n",
            "\n",
            "Epoch [1/10] - Learning Rate: 0.000025\n",
            "Epoch [1/10] finished. Average Training Loss: 1.6159, Training Accuracy: 0.3654, Duration: 140.12 seconds\n",
            "Epoch [1/10] - Validation Loss: 1.5317, Validation Accuracy: 0.3750, Validation F1-score: 0.2270\n",
            "Validation F1-score improved. Saving model checkpoint to: /content/checkpoints/best_model_epoch_1_f1_0.2270.pth\n",
            "\n",
            "Epoch [2/10] - Learning Rate: 0.000025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10] finished. Average Training Loss: 1.6543, Training Accuracy: 0.1538, Duration: 140.64 seconds\n",
            "Epoch [2/10] - Validation Loss: 1.8790, Validation Accuracy: 0.1875, Validation F1-score: 0.0833\n",
            "\n",
            "Epoch [3/10] - Learning Rate: 0.000025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10] finished. Average Training Loss: 1.6057, Training Accuracy: 0.2500, Duration: 142.60 seconds\n",
            "Epoch [3/10] - Validation Loss: 1.7279, Validation Accuracy: 0.3125, Validation F1-score: 0.1645\n",
            "\n",
            "Epoch [4/10] - Learning Rate: 0.000025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10] finished. Average Training Loss: 1.5904, Training Accuracy: 0.2115, Duration: 140.26 seconds\n",
            "Epoch [4/10] - Validation Loss: 2.0386, Validation Accuracy: 0.3750, Validation F1-score: 0.2500\n",
            "Validation F1-score improved. Saving model checkpoint to: /content/checkpoints/best_model_epoch_4_f1_0.2500.pth\n",
            "\n",
            "Epoch [5/10] - Learning Rate: 0.000025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10] finished. Average Training Loss: 1.6287, Training Accuracy: 0.2308, Duration: 142.14 seconds\n",
            "Epoch [5/10] - Validation Loss: 1.9089, Validation Accuracy: 0.4375, Validation F1-score: 0.3083\n",
            "Validation F1-score improved. Saving model checkpoint to: /content/checkpoints/best_model_epoch_5_f1_0.3083.pth\n",
            "\n",
            "Epoch [6/10] - Learning Rate: 0.000013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10] finished. Average Training Loss: 1.6116, Training Accuracy: 0.2885, Duration: 142.07 seconds\n",
            "Epoch [6/10] - Validation Loss: 1.8560, Validation Accuracy: 0.2500, Validation F1-score: 0.1500\n",
            "\n",
            "Epoch [7/10] - Learning Rate: 0.000013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10] finished. Average Training Loss: 1.6463, Training Accuracy: 0.2115, Duration: 143.07 seconds\n",
            "Epoch [7/10] - Validation Loss: 1.7280, Validation Accuracy: 0.1875, Validation F1-score: 0.0662\n",
            "\n",
            "Epoch [8/10] - Learning Rate: 0.000013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10] finished. Average Training Loss: 1.6240, Training Accuracy: 0.1538, Duration: 139.36 seconds\n",
            "Epoch [8/10] - Validation Loss: 1.9078, Validation Accuracy: 0.1250, Validation F1-score: 0.0441\n",
            "\n",
            "Epoch [9/10] - Learning Rate: 0.000013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10] finished. Average Training Loss: 1.6188, Training Accuracy: 0.2115, Duration: 139.08 seconds\n",
            "Epoch [9/10] - Validation Loss: 2.0431, Validation Accuracy: 0.4375, Validation F1-score: 0.2667\n",
            "\n",
            "Epoch [10/10] - Learning Rate: 0.000013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10] finished. Average Training Loss: 1.5971, Training Accuracy: 0.2308, Duration: 140.73 seconds\n",
            "Epoch [10/10] - Validation Loss: 1.9332, Validation Accuracy: 0.1250, Validation F1-score: 0.0556\n",
            "\n",
            "Training finished.\n",
            "Best validation F1-score (0.3083) achieved at epoch 5.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "595659f2"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The ResNet3D-18 architecture was successfully instantiated for 3D medical images with 1 input channel and adapted to have 5 output classes for phenotype classification.\n",
        "*   The model was moved to the specified device (e.g., GPU).\n",
        "*   `CrossEntropyLoss` was defined as the loss function for the classification task.\n",
        "*   The `AdamW` optimizer was defined with a learning rate of 1e-4 and weight decay of 1e-5.\n",
        "*   A `StepLR` learning rate scheduler was implemented to reduce the learning rate by half every 5 epochs.\n",
        "*   Training and validation loops were implemented and executed for 10 epochs.\n",
        "*   Training and validation loss were calculated and monitored per epoch.\n",
        "*   Classification metrics (Accuracy, F1-score, Precision, Recall) from `sklearn.metrics` were successfully integrated into the validation loop to evaluate model performance.\n",
        "*   Checkpointing was implemented to save the model's state dictionary whenever the validation F1-score improved.\n",
        "*   No pre-trained weights were loaded as the specified file was not found.\n",
        "*   Dummy labels were used throughout the process due to the absence of actual phenotype labels in the provided data structure, which is a limitation for evaluating true model performance.\n",
        "*   An `UndefinedMetricWarning` for Precision was observed during validation, likely due to the use of dummy labels or potential class imbalance, indicating that some classes might not have received any predictions.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Integrate actual phenotype labels into the data loading process to train and evaluate the model on real data.\n",
        "*   Investigate the cause of the `UndefinedMetricWarning` for Precision by analyzing class distribution in the dataset and the model's predictions per class.\n"
      ]
    }
  ]
}